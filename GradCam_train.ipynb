{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10277604571849323397\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3876174023390686652\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9396096148592444119\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5185470464\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10710404037425647570\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,TimeDistributed\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import layers,models\n",
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import heapq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScaleLayer(Layer):\n",
    "\n",
    "    def __init__(self,scale, **kwargs):\n",
    "\n",
    "        self.scale=scale;\n",
    "        \n",
    "        super(ScaleLayer, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ScaleLayer, self).build(input_shape)\n",
    "\n",
    "       \n",
    "    def call(self, inputs):\n",
    "\n",
    "        output=tf.image.resize(inputs,[self.scale,self.scale],method=tf.image.ResizeMethod.BICUBIC)\n",
    "\n",
    "\n",
    "        print(output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "       \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],self.scale,self.scale,3)\n",
    "\n",
    "\n",
    "\n",
    "def vgg16_head(img_input):\n",
    "\n",
    "    pooling = 'No'\n",
    "\n",
    "    include_top = False;\n",
    "\n",
    "\n",
    "\n",
    "    x = TimeDistributed(ScaleLayer(224))(img_input)\n",
    "\n",
    "    x = TimeDistributed(layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv2'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = TimeDistributed(layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = TimeDistributed(layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv2'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv3'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv2'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv3'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv2'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv3'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = layers.Flatten(name='flatten')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "        elif pooling =='No':\n",
    "            x = x;\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        \n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "\n",
    "        self.input_shape_r = input_shape;\n",
    "        print('self.input_shape_r')\n",
    "        print(self.input_shape_r)\n",
    "\n",
    "        L_size = 256;\n",
    "\n",
    "        self.w = self.add_weight(name='w', \n",
    "                              shape=[L_size,1],\n",
    "                              initializer='uniform',\n",
    "                              trainable=True)\n",
    "        self.V = self.add_weight(name='V', \n",
    "                      shape=[input_shape[2],L_size],\n",
    "                      initializer='uniform',\n",
    "                      trainable=True)\n",
    "\n",
    "        self.U = self.add_weight(name='U', \n",
    "                      shape=[input_shape[2],L_size],\n",
    "                      initializer='uniform',\n",
    "                      trainable=True)\n",
    "\n",
    "        \n",
    "\n",
    "        self.bias_w = self.add_weight(shape=[1,1],\n",
    "                              initializer='uniform',\n",
    "                              name='bias_w')\n",
    "        self.bias_V = self.add_weight(shape=[1,L_size],\n",
    "                                      initializer='uniform',\n",
    "                                      name='bias_V')\n",
    "        self.bias_U = self.add_weight(shape=[1,L_size],\n",
    "                                      initializer='uniform',\n",
    "                                      name='bias_U')\n",
    "        \n",
    "\n",
    "  \n",
    "       \n",
    "    def call(self, inputs):\n",
    "\n",
    "\n",
    "        inputs_r = K.expand_dims(inputs,axis=-2)\n",
    "\n",
    "\n",
    "        print('inputs')\n",
    "        print(inputs_r.shape)\n",
    "\n",
    "\n",
    "        dot_products_V = K.dot(inputs_r,self.V)\n",
    "        dot_products_U = K.dot(inputs_r,self.U)\n",
    "\n",
    "        dot_products_V += self.bias_V;\n",
    "        dot_products_U += self.bias_U;\n",
    "\n",
    "\n",
    "        print('dot product')\n",
    "        print(dot_products_V.shape)\n",
    "\n",
    "        tanh_V = K.tanh(dot_products_V);\n",
    "        sigmoid_U = K.sigmoid(dot_products_U);\n",
    "\n",
    "        print('tanh')\n",
    "        print(tanh_V.shape)\n",
    "\n",
    "        comb_UV = tanh_V*sigmoid_U;\n",
    "\n",
    "        print('combUV')\n",
    "        print(comb_UV.shape)\n",
    "\n",
    "        attention_weights = K.dot(comb_UV,self.w);\n",
    "\n",
    "        attention_weights +=self.bias_w;\n",
    "\n",
    "        attention_weights = K.exp(attention_weights);\n",
    "\n",
    "\n",
    "        print('attention_weights')\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "\n",
    "        attention_weights /= K.cast(K.sum(attention_weights,\n",
    "                                  axis=1,\n",
    "                                  keepdims=True) + K.epsilon(),\n",
    "                            K.floatx());\n",
    "        print('attention_weights sum')\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        #attention_weights = K.expand_dims(attention_weights);\n",
    "        attention_weights= K.squeeze(attention_weights,axis = -1);\n",
    "        print('attention_weights last')\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        weighted_output = inputs * attention_weights;\n",
    "\n",
    "        self.Save =  attention_weights;\n",
    "\n",
    "\n",
    "        output = K.sum(weighted_output, axis=1)\n",
    "\n",
    "        print('output')\n",
    "        print(output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[2])\n",
    "    \n",
    "    \n",
    "class FCHeadNet:\n",
    "  @staticmethod\n",
    "  def build(baseModel, classes, D):\n",
    "    # initialize the head model that will be placed on top of\n",
    "    # the base, then add a FC layer\n",
    "    headModel = baseModel.output\n",
    "    headModel = TimeDistributed(layers.Flatten(name='flatten'))(headModel)\n",
    "    headModel = TimeDistributed(layers.Dense(D, activation='relu', name='fc1'))(headModel)\n",
    "    headModel = TimeDistributed(layers.Dense(D, activation='relu', name='fc2'))(headModel)\n",
    "    headModel = AttentionLayer(name='attentionlayer')(headModel)\n",
    "    headModel = layers.Dense(classes, activation='softmax', name='predictions')(headModel)\n",
    "    # add a softmax layer\n",
    "    #headModel = layers.Dense(classes, activation=\"softmax\")(headModel)\n",
    "    #headModel = layers.Dense(classes, activation='softmax', name='predictions2')(headModel)\n",
    "\n",
    "    # return the model\n",
    "    return headModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160, 128, 128, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 10, 128, 128, 3)\n",
      "(116,)\n",
      "(87, 10, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "batch_size = 1\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 128, 128\n",
    "\n",
    "# the data, split between train and test sets\n",
    "\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "p='/home/peppermint/Data_Sci/Attention_ML/1160_many.h5';\n",
    "\n",
    "db=h5py.File(p);\n",
    "\n",
    "\n",
    "X_stk, y_stk = db['images'],db['labels'];\n",
    "\n",
    "X =np.zeros((116,10,128,128,3))\n",
    "y = np.zeros((116,))\n",
    "\n",
    "for ij in range(116):\n",
    "    X[ij,...] = X_stk[ij*10:(ij+1)*10,...]\n",
    "    y[ij,...] = y_stk[(ij*10+(ij+1)*10)//2]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "total_size = X.shape[0];\n",
    "index_random = np.arange(total_size)\n",
    "i = int(X.shape[0] * 3/4)\n",
    "\n",
    "random.shuffle(index_random)\n",
    "\n",
    "train_index = list(index_random[:i]) ;\n",
    "test_index = list(index_random[i:]) ;\n",
    "train_index.sort()\n",
    "test_index.sort()\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = X[train_index],X[test_index],y[train_index],y[test_index];\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0504 14:01:53.928223 140002851256128 deprecation_wrapper.py:119] From /home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0504 14:01:57.601845 140002851256128 deprecation.py:323] From /home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.input_shape_r\n",
      "(None, 10, 256)\n",
      "inputs\n",
      "(?, 10, 1, 256)\n",
      "dot product\n",
      "(?, 10, 1, 256)\n",
      "tanh\n",
      "(?, 10, 1, 256)\n",
      "combUV\n",
      "(?, 10, 1, 256)\n",
      "attention_weights\n",
      "(?, 10, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 10, 1, 1)\n",
      "attention_weights last\n",
      "(?, 10, 1)\n",
      "output\n",
      "(?, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0504 14:01:58.252212 140002851256128 deprecation_wrapper.py:119] From /home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87 samples, validate on 29 samples\n",
      "Epoch 1/50\n",
      "87/87 [==============================] - 15s 172ms/step - loss: 0.6680 - accuracy: 0.6935 - val_loss: 0.5486 - val_accuracy: 0.7126\n",
      "Epoch 2/50\n",
      "87/87 [==============================] - 9s 105ms/step - loss: 0.5147 - accuracy: 0.7548 - val_loss: 0.4912 - val_accuracy: 0.7126\n",
      "Epoch 3/50\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 0.3956 - accuracy: 0.8314 - val_loss: 0.5468 - val_accuracy: 0.7241\n",
      "Epoch 4/50\n",
      "87/87 [==============================] - 8s 98ms/step - loss: 0.3694 - accuracy: 0.8582 - val_loss: 0.3937 - val_accuracy: 0.8161\n",
      "Epoch 5/50\n",
      "87/87 [==============================] - 9s 98ms/step - loss: 0.2910 - accuracy: 0.8544 - val_loss: 0.4648 - val_accuracy: 0.7701\n",
      "Epoch 6/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.2247 - accuracy: 0.9310 - val_loss: 0.4176 - val_accuracy: 0.7931\n",
      "Epoch 7/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.2172 - accuracy: 0.9004 - val_loss: 0.3206 - val_accuracy: 0.8506\n",
      "Epoch 8/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.1487 - accuracy: 0.9540 - val_loss: 0.3664 - val_accuracy: 0.8276\n",
      "Epoch 9/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0955 - accuracy: 0.9770 - val_loss: 0.5271 - val_accuracy: 0.7816\n",
      "Epoch 10/50\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 0.0896 - accuracy: 0.9808 - val_loss: 0.2682 - val_accuracy: 0.8621\n",
      "Epoch 11/50\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 0.0620 - accuracy: 0.9808 - val_loss: 0.3157 - val_accuracy: 0.8736\n",
      "Epoch 12/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0554 - accuracy: 0.9847 - val_loss: 0.3343 - val_accuracy: 0.8391\n",
      "Epoch 13/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.8851\n",
      "Epoch 14/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.3148 - val_accuracy: 0.8621\n",
      "Epoch 15/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0338 - accuracy: 0.9923 - val_loss: 0.4465 - val_accuracy: 0.8276\n",
      "Epoch 16/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.3489 - val_accuracy: 0.8621\n",
      "Epoch 17/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.3371 - val_accuracy: 0.8621\n",
      "Epoch 18/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.3789 - val_accuracy: 0.8391\n",
      "Epoch 19/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.3853 - val_accuracy: 0.8621\n",
      "Epoch 20/50\n",
      "87/87 [==============================] - 9s 100ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.3477 - val_accuracy: 0.8621\n",
      "Epoch 21/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4276 - val_accuracy: 0.8621\n",
      "Epoch 22/50\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3688 - val_accuracy: 0.8621\n",
      "Epoch 23/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3737 - val_accuracy: 0.8621\n",
      "Epoch 24/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3990 - val_accuracy: 0.8621\n",
      "Epoch 25/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4289 - val_accuracy: 0.8621\n",
      "Epoch 26/50\n",
      "87/87 [==============================] - 9s 108ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4170 - val_accuracy: 0.8621\n",
      "Epoch 27/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4194 - val_accuracy: 0.8621\n",
      "Epoch 28/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4409 - val_accuracy: 0.8621\n",
      "Epoch 29/50\n",
      "87/87 [==============================] - 9s 106ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4336 - val_accuracy: 0.8621\n",
      "Epoch 30/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4403 - val_accuracy: 0.8621\n",
      "Epoch 31/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4740 - val_accuracy: 0.8621\n",
      "Epoch 32/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4706 - val_accuracy: 0.8621\n",
      "Epoch 33/50\n",
      "87/87 [==============================] - 9s 108ms/step - loss: 9.8255e-04 - accuracy: 1.0000 - val_loss: 0.4750 - val_accuracy: 0.8621\n",
      "Epoch 34/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 9.0889e-04 - accuracy: 1.0000 - val_loss: 0.4716 - val_accuracy: 0.8621\n",
      "Epoch 35/50\n",
      "87/87 [==============================] - 9s 108ms/step - loss: 8.6445e-04 - accuracy: 1.0000 - val_loss: 0.4800 - val_accuracy: 0.8621\n",
      "Epoch 36/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 7.6990e-04 - accuracy: 1.0000 - val_loss: 0.4899 - val_accuracy: 0.8621\n",
      "Epoch 37/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 7.0947e-04 - accuracy: 1.0000 - val_loss: 0.5009 - val_accuracy: 0.8621\n",
      "Epoch 38/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 6.5649e-04 - accuracy: 1.0000 - val_loss: 0.5056 - val_accuracy: 0.8621\n",
      "Epoch 39/50\n",
      "87/87 [==============================] - 9s 108ms/step - loss: 5.8829e-04 - accuracy: 1.0000 - val_loss: 0.5050 - val_accuracy: 0.8621\n",
      "Epoch 40/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 5.4691e-04 - accuracy: 1.0000 - val_loss: 0.5378 - val_accuracy: 0.8621\n",
      "Epoch 41/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 4.8743e-04 - accuracy: 1.0000 - val_loss: 0.5334 - val_accuracy: 0.8621\n",
      "Epoch 42/50\n",
      "87/87 [==============================] - 9s 108ms/step - loss: 4.5546e-04 - accuracy: 1.0000 - val_loss: 0.5609 - val_accuracy: 0.8621\n",
      "Epoch 43/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 4.2194e-04 - accuracy: 1.0000 - val_loss: 0.5465 - val_accuracy: 0.8621\n",
      "Epoch 44/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 3.9370e-04 - accuracy: 1.0000 - val_loss: 0.5654 - val_accuracy: 0.8621\n",
      "Epoch 45/50\n",
      "87/87 [==============================] - 9s 108ms/step - loss: 3.5749e-04 - accuracy: 1.0000 - val_loss: 0.5799 - val_accuracy: 0.8621\n",
      "Epoch 46/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 3.3839e-04 - accuracy: 1.0000 - val_loss: 0.5748 - val_accuracy: 0.8621\n",
      "Epoch 47/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 3.1672e-04 - accuracy: 1.0000 - val_loss: 0.5989 - val_accuracy: 0.8621\n",
      "Epoch 48/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 2.8983e-04 - accuracy: 1.0000 - val_loss: 0.5848 - val_accuracy: 0.8621\n",
      "Epoch 49/50\n",
      "87/87 [==============================] - 9s 108ms/step - loss: 2.6894e-04 - accuracy: 1.0000 - val_loss: 0.5978 - val_accuracy: 0.8621\n",
      "Epoch 50/50\n",
      "87/87 [==============================] - 9s 107ms/step - loss: 2.5253e-04 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 0.8621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f54cf26aac8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rows, img_cols = 128, 128\n",
    "\n",
    "pooling = 'No'\n",
    "epochs = 50\n",
    "\n",
    "num_classes = 3\n",
    "num_sequence = 10;\n",
    "include_top = False;\n",
    "\n",
    "input_shape =  [num_sequence,img_rows, img_cols, 3];\n",
    "img_input = layers.Input(shape=input_shape)\n",
    "inputs = img_input\n",
    "\n",
    "\n",
    "x = vgg16_head(inputs);\n",
    "\n",
    "# Create model.\n",
    "base_model = models.Model(inputs, x, name='vgg16')\n",
    "\n",
    "weights ='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5';\n",
    "\n",
    "base_model.load_weights(weights)\n",
    "\n",
    "\n",
    "head_model = FCHeadNet.build(base_model, num_classes, 256)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=head_model)\n",
    "\n",
    "Dont_Want_to_train_all = True;\n",
    "\n",
    "\n",
    "if (Dont_Want_to_train_all):\n",
    "\n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./\"\n",
    "\n",
    "\n",
    "weights_dir = save_dir + \"weights_lung.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_current = weights_dir\n",
    "\n",
    "\n",
    "model.load_weights(model_dir_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 13/50\n",
    " - 9s 99ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.8851"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
