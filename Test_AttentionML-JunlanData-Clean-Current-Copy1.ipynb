{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2643237401841409991\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3680104960388383176\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5944326084714759917\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5618860032\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1387142076178331006\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,TimeDistributed\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import layers,models\n",
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import heapq\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScaleLayer(Layer):\n",
    "\n",
    "    def __init__(self,scale, **kwargs):\n",
    "\n",
    "        self.scale=scale;\n",
    "        \n",
    "        super(ScaleLayer, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ScaleLayer, self).build(input_shape)\n",
    "\n",
    "       \n",
    "    def call(self, inputs):\n",
    "\n",
    "        output=tf.image.resize(inputs,[self.scale,self.scale],method=tf.image.ResizeMethod.BICUBIC)\n",
    "\n",
    "\n",
    "        print(output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "       \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],self.scale,self.scale,3)\n",
    "\n",
    "\n",
    "\n",
    "def vgg16_head(img_input):\n",
    "\n",
    "    pooling = 'No'\n",
    "\n",
    "    include_top = False;\n",
    "\n",
    "\n",
    "\n",
    "    x = TimeDistributed(ScaleLayer(224))(img_input)\n",
    "\n",
    "    x = TimeDistributed(layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv2'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = TimeDistributed(layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = TimeDistributed(layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv2'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv3'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv2'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv3'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv2'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv3'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = layers.Flatten(name='flatten')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "        elif pooling =='No':\n",
    "            x = x;\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        \n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "\n",
    "        self.input_shape_r = input_shape;\n",
    "        print('self.input_shape_r')\n",
    "        print(self.input_shape_r)\n",
    "\n",
    "        L_size = 256;\n",
    "\n",
    "        self.w = self.add_weight(name='w', \n",
    "                              shape=[L_size,1],\n",
    "                              initializer='uniform',\n",
    "                              trainable=True)\n",
    "        self.V = self.add_weight(name='V', \n",
    "                      shape=[input_shape[2],L_size],\n",
    "                      initializer='uniform',\n",
    "                      trainable=True)\n",
    "\n",
    "        self.U = self.add_weight(name='U', \n",
    "                      shape=[input_shape[2],L_size],\n",
    "                      initializer='uniform',\n",
    "                      trainable=True)\n",
    "\n",
    "        \n",
    "\n",
    "        self.bias_w = self.add_weight(shape=[1,1],\n",
    "                              initializer='uniform',\n",
    "                              name='bias_w')\n",
    "        self.bias_V = self.add_weight(shape=[1,L_size],\n",
    "                                      initializer='uniform',\n",
    "                                      name='bias_V')\n",
    "        self.bias_U = self.add_weight(shape=[1,L_size],\n",
    "                                      initializer='uniform',\n",
    "                                      name='bias_U')\n",
    "        \n",
    "        self.Save = self.add_weight(name='ForSave', \n",
    "              shape=[10,10],\n",
    "              initializer='uniform',\n",
    "              trainable=False)\n",
    "      \n",
    "\n",
    "  \n",
    "       \n",
    "    def call(self, inputs):\n",
    "\n",
    "\n",
    "        inputs_r = K.expand_dims(inputs,axis=-2)\n",
    "\n",
    "\n",
    "        print('inputs')\n",
    "        print(inputs_r.shape)\n",
    "\n",
    "\n",
    "        dot_products_V = K.dot(inputs_r,self.V)\n",
    "        dot_products_U = K.dot(inputs_r,self.U)\n",
    "\n",
    "        dot_products_V += self.bias_V;\n",
    "        dot_products_U += self.bias_U;\n",
    "\n",
    "\n",
    "        print('dot product')\n",
    "        print(dot_products_V.shape)\n",
    "\n",
    "        tanh_V = K.tanh(dot_products_V);\n",
    "        sigmoid_U = K.sigmoid(dot_products_U);\n",
    "\n",
    "        print('tanh')\n",
    "        print(tanh_V.shape)\n",
    "\n",
    "        comb_UV = tanh_V*sigmoid_U;\n",
    "\n",
    "        print('combUV')\n",
    "        print(comb_UV.shape)\n",
    "\n",
    "        attention_weights = K.dot(comb_UV,self.w);\n",
    "\n",
    "        attention_weights +=self.bias_w;\n",
    "\n",
    "        attention_weights = K.exp(attention_weights);\n",
    "\n",
    "\n",
    "        print('attention_weights')\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "\n",
    "        attention_weights /= K.cast(K.sum(attention_weights,\n",
    "                                  axis=1,\n",
    "                                  keepdims=True) + K.epsilon(),\n",
    "                            K.floatx());\n",
    "        print('attention_weights sum')\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        #attention_weights = K.expand_dims(attention_weights);\n",
    "        attention_weights= K.squeeze(attention_weights,axis = -1);\n",
    "        print('attention_weights last')\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        weighted_output = inputs * attention_weights;\n",
    "\n",
    "        self.Save =  attention_weights;\n",
    "\n",
    "\n",
    "        output = K.sum(weighted_output, axis=1)\n",
    "\n",
    "        print('output')\n",
    "        print(output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[2])\n",
    "    \n",
    "    \n",
    "class FCHeadNet:\n",
    "  @staticmethod\n",
    "  def build(baseModel, classes, D):\n",
    "    # initialize the head model that will be placed on top of\n",
    "    # the base, then add a FC layer\n",
    "    headModel = baseModel.output\n",
    "    headModel = TimeDistributed(layers.Flatten(name='flatten'))(headModel)\n",
    "    headModel = TimeDistributed(layers.Dense(D, activation='relu', name='fc1'))(headModel)\n",
    "    headModel = TimeDistributed(layers.Dense(D, activation='relu', name='fc2'))(headModel)\n",
    "    headModel = AttentionLayer(name='attentionlayer')(headModel)\n",
    "    headModel = layers.Dense(classes, activation='sigmoid', name='predictions')(headModel)\n",
    "    # add a softmax layer\n",
    "    #headModel = layers.Dense(classes, activation=\"softmax\")(headModel)\n",
    "    #headModel = layers.Dense(classes, activation='softmax', name='predictions2')(headModel)\n",
    "\n",
    "    # return the model\n",
    "    return headModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 28, 128, 128, 3)\n",
      "(91,)\n",
      "[63, 3, 23, 86, 47, 54, 31, 32, 45, 24, 57, 16, 1, 12, 11, 34, 43, 10, 26, 84, 2, 42, 21, 44, 66, 89, 49, 35, 28, 27, 30, 55, 79, 53, 78, 9, 67, 0, 62, 13, 85, 82, 46, 17, 7, 20, 72, 41, 74, 48, 68, 70, 73, 39, 52, 5, 8, 40, 64, 76, 33, 38, 61, 71, 90, 51, 22, 83, 88, 15, 14, 69, 29, 50, 58, 4, 81, 37, 87, 6, 65, 56, 19, 60, 59, 80, 77, 25, 36, 18, 75]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "batch_size = 1\n",
    "num_classes = 2\n",
    "epochs = 100\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 128, 128\n",
    "\n",
    "# the data, split between train and test sets\n",
    "\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "p='/home/peppermint/Data_Sci/Attention_ML/2548_many.h5';\n",
    "\n",
    "db=h5py.File(p);\n",
    "\n",
    "\n",
    "X_stk, y_stk = db['images'],db['labels'];\n",
    "\n",
    "X =np.zeros((91,28,128,128,3))\n",
    "y = np.zeros((91,))\n",
    "\n",
    "for ij in range(91):\n",
    "    X[ij,...] = X_stk[ij*28:(ij+1)*28,...]\n",
    "    y[ij,...] = y_stk[(ij*28+(ij+1)*28)//2]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "total_size = X.shape[0];\n",
    "index_random = np.arange(total_size)\n",
    "i = int(X.shape[0] * 3/4)\n",
    "\n",
    "random.shuffle(index_random)\n",
    "\n",
    "index_random=list(index_random)\n",
    "\n",
    "X = X[list(index_random)]\n",
    "\n",
    "y = y[list(index_random)]\n",
    "\n",
    "\n",
    "\n",
    "print(list(index_random))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.4511502955115987e-15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X[0,0,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 28, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "X_mask = (X[...,0]>0.2) | (X[...,1]>0.2) | (X[...,2]>0.2)\n",
    "X_mask  = X_mask*1\n",
    "X_mask = X_mask[...,None]\n",
    "X_mask =np.concatenate([X_mask,X_mask,X_mask],axis=-1)\n",
    "print(X_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa2c12d2b70>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_mask[0,0,...]*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0428 00:57:56.250262 139842270504768 deprecation_wrapper.py:119] From /home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72 samples, validate on 19 samples\n",
      "Epoch 1/50\n",
      "72/72 [==============================] - 18s 253ms/step - loss: 0.7844 - accuracy: 0.5972 - val_loss: 0.6899 - val_accuracy: 0.5263\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.6261 - accuracy: 0.6389 - val_loss: 0.7229 - val_accuracy: 0.5263\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6438 - accuracy: 0.6111 - val_loss: 0.6459 - val_accuracy: 0.6316\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.5708 - accuracy: 0.6806 - val_loss: 0.6051 - val_accuracy: 0.6842\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.5705 - accuracy: 0.7361 - val_loss: 0.5920 - val_accuracy: 0.7368\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.5394 - accuracy: 0.7222 - val_loss: 0.5724 - val_accuracy: 0.7368\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.4759 - accuracy: 0.7778 - val_loss: 0.5826 - val_accuracy: 0.6842\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.4357 - accuracy: 0.8194 - val_loss: 0.6117 - val_accuracy: 0.6842\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.4335 - accuracy: 0.8056 - val_loss: 0.5185 - val_accuracy: 0.7368\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.2883 - accuracy: 0.9028 - val_loss: 0.5785 - val_accuracy: 0.7368\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.2476 - accuracy: 0.9028 - val_loss: 0.5601 - val_accuracy: 0.7368\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.1840 - accuracy: 0.9444 - val_loss: 0.5370 - val_accuracy: 0.8421\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.1571 - accuracy: 0.9444 - val_loss: 0.6923 - val_accuracy: 0.7368\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.1660 - accuracy: 0.9583 - val_loss: 0.9298 - val_accuracy: 0.6316\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0834 - accuracy: 0.9861 - val_loss: 0.4479 - val_accuracy: 0.7368\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 15s 207ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.4119 - val_accuracy: 0.8421\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 0.7368\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.4564 - val_accuracy: 0.8421\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.6897 - val_accuracy: 0.7895\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 0.7895\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.4801 - val_accuracy: 0.8421\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3983 - val_accuracy: 0.8421\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6666 - val_accuracy: 0.7895\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.8421\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.4859 - val_accuracy: 0.8421\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.5573 - val_accuracy: 0.8421\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5543 - val_accuracy: 0.8421\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5630 - val_accuracy: 0.8421\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4674 - val_accuracy: 0.8421\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5089 - val_accuracy: 0.8421\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5784 - val_accuracy: 0.8421\n",
      "Epoch 32/50\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5292 - val_accuracy: 0.8421\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 15s 201ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5482 - val_accuracy: 0.8421\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.7004 - val_accuracy: 0.7895\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5858 - val_accuracy: 0.8421\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5835 - val_accuracy: 0.8421\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6156 - val_accuracy: 0.8421\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5493 - val_accuracy: 0.8421\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5671 - val_accuracy: 0.8421\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5395 - val_accuracy: 0.8421\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5667 - val_accuracy: 0.8421\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6348 - val_accuracy: 0.8421\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 9.4240e-04 - accuracy: 1.0000 - val_loss: 0.5746 - val_accuracy: 0.8421\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 8.9508e-04 - accuracy: 1.0000 - val_loss: 0.5869 - val_accuracy: 0.8421\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 8.2826e-04 - accuracy: 1.0000 - val_loss: 0.6304 - val_accuracy: 0.8421\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 8.0077e-04 - accuracy: 1.0000 - val_loss: 0.5892 - val_accuracy: 0.8421\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 7.2680e-04 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 0.8421\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 15s 201ms/step - loss: 6.8333e-04 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 0.8421\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 6.3721e-04 - accuracy: 1.0000 - val_loss: 0.6399 - val_accuracy: 0.8421\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 5.8505e-04 - accuracy: 1.0000 - val_loss: 0.7255 - val_accuracy: 0.7895\n",
      "Epoch 1\n",
      "max_val :0.8421052694320679\n",
      "save_acc :1.0\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 15s 204ms/step - loss: 0.8585 - accuracy: 0.5616 - val_loss: 0.6466 - val_accuracy: 0.6667\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 15s 201ms/step - loss: 0.6575 - accuracy: 0.5890 - val_loss: 0.6154 - val_accuracy: 0.6667\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 14s 198ms/step - loss: 0.5860 - accuracy: 0.6712 - val_loss: 0.6001 - val_accuracy: 0.6111\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 15s 200ms/step - loss: 0.5582 - accuracy: 0.7534 - val_loss: 0.6074 - val_accuracy: 0.5556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "73/73 [==============================] - 15s 201ms/step - loss: 0.5129 - accuracy: 0.7534 - val_loss: 0.6169 - val_accuracy: 0.7222\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 15s 201ms/step - loss: 0.4517 - accuracy: 0.7397 - val_loss: 0.6249 - val_accuracy: 0.7222\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 15s 200ms/step - loss: 0.4058 - accuracy: 0.7945 - val_loss: 0.6482 - val_accuracy: 0.6111\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 15s 200ms/step - loss: 0.4504 - accuracy: 0.7945 - val_loss: 0.6409 - val_accuracy: 0.6667\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 15s 204ms/step - loss: 0.3762 - accuracy: 0.8356 - val_loss: 0.9523 - val_accuracy: 0.4444\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.3138 - accuracy: 0.8493 - val_loss: 0.6738 - val_accuracy: 0.6111\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.2492 - accuracy: 0.9315 - val_loss: 0.5070 - val_accuracy: 0.7778\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.2206 - accuracy: 0.9178 - val_loss: 0.5585 - val_accuracy: 0.7222\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.1694 - accuracy: 0.9178 - val_loss: 0.6328 - val_accuracy: 0.6111\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.1212 - accuracy: 0.9863 - val_loss: 0.6200 - val_accuracy: 0.6111\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0728 - accuracy: 0.9863 - val_loss: 0.6226 - val_accuracy: 0.5556\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0724 - accuracy: 1.0000 - val_loss: 0.4114 - val_accuracy: 0.8333\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.4963 - val_accuracy: 0.7222\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0438 - accuracy: 0.9863 - val_loss: 0.4289 - val_accuracy: 0.7778\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.6111\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.5068 - val_accuracy: 0.7222\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.3901 - val_accuracy: 0.8333\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.5988 - val_accuracy: 0.6111\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.5005 - val_accuracy: 0.8333\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 15s 204ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.5748 - val_accuracy: 0.7778\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4817 - val_accuracy: 0.8333\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5317 - val_accuracy: 0.8333\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 15s 204ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.5768 - val_accuracy: 0.8333\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 0.8333\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7849 - val_accuracy: 0.6111\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.6402 - val_accuracy: 0.8333\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5574 - val_accuracy: 0.8333\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5095 - val_accuracy: 0.8333\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6573 - val_accuracy: 0.8333\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5704 - val_accuracy: 0.8333\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6584 - val_accuracy: 0.8333\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6626 - val_accuracy: 0.8333\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5996 - val_accuracy: 0.8333\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.8333\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 15s 201ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6627 - val_accuracy: 0.8333\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6575 - val_accuracy: 0.8333\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.8333\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6471 - val_accuracy: 0.8333\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 9.8996e-04 - accuracy: 1.0000 - val_loss: 0.7029 - val_accuracy: 0.8333\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 9.5927e-04 - accuracy: 1.0000 - val_loss: 0.6838 - val_accuracy: 0.8333\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 8.6713e-04 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.8333\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 15s 204ms/step - loss: 7.8558e-04 - accuracy: 1.0000 - val_loss: 0.6636 - val_accuracy: 0.8333\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 7.2129e-04 - accuracy: 1.0000 - val_loss: 0.7508 - val_accuracy: 0.8333\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 15s 205ms/step - loss: 6.8812e-04 - accuracy: 1.0000 - val_loss: 0.6699 - val_accuracy: 0.8333\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 6.6308e-04 - accuracy: 1.0000 - val_loss: 0.6431 - val_accuracy: 0.8333\n",
      "Epoch 2\n",
      "max_val :0.8333333134651184\n",
      "save_acc :1.0\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 17s 237ms/step - loss: 0.8959 - accuracy: 0.5479 - val_loss: 0.6893 - val_accuracy: 0.6667\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.6763 - accuracy: 0.5753 - val_loss: 0.6671 - val_accuracy: 0.5556\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.6111 - accuracy: 0.7123 - val_loss: 0.5918 - val_accuracy: 0.7222\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.5927 - accuracy: 0.6986 - val_loss: 0.5938 - val_accuracy: 0.6111\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 17s 230ms/step - loss: 0.4947 - accuracy: 0.7808 - val_loss: 0.6425 - val_accuracy: 0.7222\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.4820 - accuracy: 0.7945 - val_loss: 0.6944 - val_accuracy: 0.6111\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.4155 - accuracy: 0.7945 - val_loss: 0.5923 - val_accuracy: 0.6111\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.3238 - accuracy: 0.9041 - val_loss: 0.6254 - val_accuracy: 0.7778\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.2798 - accuracy: 0.8767 - val_loss: 0.5818 - val_accuracy: 0.6111\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.2701 - accuracy: 0.9041 - val_loss: 0.5705 - val_accuracy: 0.6667\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.2421 - accuracy: 0.8904 - val_loss: 0.6609 - val_accuracy: 0.6667\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.1437 - accuracy: 0.9726 - val_loss: 0.6479 - val_accuracy: 0.6111\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.1114 - accuracy: 0.9863 - val_loss: 0.6078 - val_accuracy: 0.6667\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0921 - accuracy: 0.9589 - val_loss: 0.8422 - val_accuracy: 0.7222\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0622 - accuracy: 0.9863 - val_loss: 0.7000 - val_accuracy: 0.6111\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.7021 - val_accuracy: 0.6111\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.6900 - val_accuracy: 0.6667\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.6947 - val_accuracy: 0.6667\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.6967 - val_accuracy: 0.6667\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.7035 - val_accuracy: 0.6667\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.7500 - val_accuracy: 0.7778\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.7315 - val_accuracy: 0.6667\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.7300 - val_accuracy: 0.6667\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.7649 - val_accuracy: 0.6667\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.7700 - val_accuracy: 0.7222\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 16s 222ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.7690 - val_accuracy: 0.6667\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.7855 - val_accuracy: 0.6667\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7960 - val_accuracy: 0.7778\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.7823 - val_accuracy: 0.6667\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.7951 - val_accuracy: 0.6667\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.8247 - val_accuracy: 0.6667\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.8071 - val_accuracy: 0.7778\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.8057 - val_accuracy: 0.6667\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8095 - val_accuracy: 0.6667\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8212 - val_accuracy: 0.6667\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8255 - val_accuracy: 0.7778\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8333 - val_accuracy: 0.6667\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8410 - val_accuracy: 0.7778\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8629 - val_accuracy: 0.6667\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8464 - val_accuracy: 0.7778\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8716 - val_accuracy: 0.6667\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8710 - val_accuracy: 0.6667\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8629 - val_accuracy: 0.6667\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.8567 - val_accuracy: 0.7778\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 9.6036e-04 - accuracy: 1.0000 - val_loss: 0.8758 - val_accuracy: 0.8333\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 9.4979e-04 - accuracy: 1.0000 - val_loss: 0.8725 - val_accuracy: 0.7778\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 8.2068e-04 - accuracy: 1.0000 - val_loss: 0.8745 - val_accuracy: 0.7778\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 7.3327e-04 - accuracy: 1.0000 - val_loss: 0.8804 - val_accuracy: 0.7222\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 7.2426e-04 - accuracy: 1.0000 - val_loss: 0.8832 - val_accuracy: 0.7778\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 6.6119e-04 - accuracy: 1.0000 - val_loss: 0.8914 - val_accuracy: 0.7222\n",
      "Epoch 3\n",
      "max_val :0.8333333134651184\n",
      "save_acc :1.0\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 17s 237ms/step - loss: 0.9012 - accuracy: 0.5753 - val_loss: 0.6429 - val_accuracy: 0.6111\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.6456 - accuracy: 0.6301 - val_loss: 0.6405 - val_accuracy: 0.6111\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 17s 232ms/step - loss: 0.6178 - accuracy: 0.6438 - val_loss: 0.6338 - val_accuracy: 0.6111\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 17s 234ms/step - loss: 0.6145 - accuracy: 0.6712 - val_loss: 0.6057 - val_accuracy: 0.6111\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 17s 234ms/step - loss: 0.5507 - accuracy: 0.6301 - val_loss: 0.5623 - val_accuracy: 0.6667\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.5580 - accuracy: 0.6986 - val_loss: 0.6227 - val_accuracy: 0.6667\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.5190 - accuracy: 0.7671 - val_loss: 0.5299 - val_accuracy: 0.7778\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.4373 - accuracy: 0.7808 - val_loss: 0.5226 - val_accuracy: 0.7222\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 17s 232ms/step - loss: 0.3874 - accuracy: 0.8356 - val_loss: 0.4898 - val_accuracy: 0.7778\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.3521 - accuracy: 0.8630 - val_loss: 0.5125 - val_accuracy: 0.7222\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.3196 - accuracy: 0.8493 - val_loss: 0.4059 - val_accuracy: 0.8333\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.2638 - accuracy: 0.9315 - val_loss: 0.4011 - val_accuracy: 0.8333\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 17s 226ms/step - loss: 0.2287 - accuracy: 0.9452 - val_loss: 0.4521 - val_accuracy: 0.7222\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.1852 - accuracy: 0.9315 - val_loss: 0.5215 - val_accuracy: 0.7778\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.1422 - accuracy: 0.9589 - val_loss: 0.3250 - val_accuracy: 0.8889\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0987 - accuracy: 1.0000 - val_loss: 0.7340 - val_accuracy: 0.6667\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0830 - accuracy: 0.9863 - val_loss: 0.2953 - val_accuracy: 0.8889\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.2880 - val_accuracy: 0.8889\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.3270 - val_accuracy: 0.8889\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.3231 - val_accuracy: 0.8889\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 17s 229ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.2898 - val_accuracy: 0.8889\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.2953 - val_accuracy: 0.8889\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.3971 - val_accuracy: 0.8889\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.3789 - val_accuracy: 0.8889\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.8889\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.3142 - val_accuracy: 0.9444\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.3284 - val_accuracy: 0.9444\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4384 - val_accuracy: 0.8889\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.3843 - val_accuracy: 0.8889\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.4829 - val_accuracy: 0.8889\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.3854 - val_accuracy: 0.9444\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4000 - val_accuracy: 0.9444\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4523 - val_accuracy: 0.8889\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4293 - val_accuracy: 0.8889\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4529 - val_accuracy: 0.8889\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4326 - val_accuracy: 0.9444\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4453 - val_accuracy: 0.9444\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4365 - val_accuracy: 0.9444\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4497 - val_accuracy: 0.9444\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4516 - val_accuracy: 0.9444\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4518 - val_accuracy: 0.9444\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4546 - val_accuracy: 0.9444\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4603 - val_accuracy: 0.9444\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4888 - val_accuracy: 0.8889\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4821 - val_accuracy: 0.9444\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4724 - val_accuracy: 0.9444\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4810 - val_accuracy: 0.9444\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.9444\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 9.1033e-04 - accuracy: 1.0000 - val_loss: 0.5378 - val_accuracy: 0.8889\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 8.6686e-04 - accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.9444\n",
      "Epoch 4\n",
      "max_val :0.9444444179534912\n",
      "save_acc :1.0\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 17s 232ms/step - loss: 0.7630 - accuracy: 0.5205 - val_loss: 0.6324 - val_accuracy: 0.6111\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.6135 - accuracy: 0.6575 - val_loss: 0.6225 - val_accuracy: 0.6667\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.6173 - accuracy: 0.6986 - val_loss: 0.5986 - val_accuracy: 0.6111\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 17s 229ms/step - loss: 0.5850 - accuracy: 0.6986 - val_loss: 0.5576 - val_accuracy: 0.7778\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.5002 - accuracy: 0.7397 - val_loss: 0.5288 - val_accuracy: 0.7778\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.4434 - accuracy: 0.8219 - val_loss: 0.6198 - val_accuracy: 0.7222\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.4401 - accuracy: 0.8219 - val_loss: 0.4579 - val_accuracy: 0.8889\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.3675 - accuracy: 0.8356 - val_loss: 0.5940 - val_accuracy: 0.7222\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.4419 - accuracy: 0.7808 - val_loss: 0.4285 - val_accuracy: 0.8333\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.3095 - accuracy: 0.8630 - val_loss: 0.3523 - val_accuracy: 0.8889\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.2520 - accuracy: 0.8630 - val_loss: 0.3567 - val_accuracy: 0.8889\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.1280 - accuracy: 0.9863 - val_loss: 0.3661 - val_accuracy: 0.8889\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.1060 - accuracy: 0.9726 - val_loss: 0.4985 - val_accuracy: 0.8333\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0611 - accuracy: 1.0000 - val_loss: 0.8727 - val_accuracy: 0.6667\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.3914 - val_accuracy: 0.8889\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 17s 232ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.7188 - val_accuracy: 0.7778\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.5175 - val_accuracy: 0.8889\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.3916 - val_accuracy: 0.8889\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.5281 - val_accuracy: 0.8889\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.5279 - val_accuracy: 0.8889\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 17s 230ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.4935 - val_accuracy: 0.8889\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.5226 - val_accuracy: 0.8889\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.4223 - val_accuracy: 0.8889\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5106 - val_accuracy: 0.8889\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.6329 - val_accuracy: 0.8333\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 0.8889\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4432 - val_accuracy: 0.8889\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6760 - val_accuracy: 0.8333\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 0.8889\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4906 - val_accuracy: 0.8889\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 17s 229ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 0.8889\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5005 - val_accuracy: 0.8889\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6060 - val_accuracy: 0.8889\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5189 - val_accuracy: 0.8889\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 0.8889\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5740 - val_accuracy: 0.8889\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.5283 - val_accuracy: 0.8889\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 9.6670e-04 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 0.8889\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 9.2798e-04 - accuracy: 1.0000 - val_loss: 0.6916 - val_accuracy: 0.8889\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 9.2425e-04 - accuracy: 1.0000 - val_loss: 0.6271 - val_accuracy: 0.8889\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 7.9752e-04 - accuracy: 1.0000 - val_loss: 0.5659 - val_accuracy: 0.8889\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 7.0264e-04 - accuracy: 1.0000 - val_loss: 0.5897 - val_accuracy: 0.8889\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 6.9932e-04 - accuracy: 1.0000 - val_loss: 0.5241 - val_accuracy: 0.8889\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 6.2117e-04 - accuracy: 1.0000 - val_loss: 0.5782 - val_accuracy: 0.8889\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 5.7763e-04 - accuracy: 1.0000 - val_loss: 0.5632 - val_accuracy: 0.8889\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 5.4207e-04 - accuracy: 1.0000 - val_loss: 0.5712 - val_accuracy: 0.8889\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 5.1325e-04 - accuracy: 1.0000 - val_loss: 0.6425 - val_accuracy: 0.8889\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 4.6559e-04 - accuracy: 1.0000 - val_loss: 0.5881 - val_accuracy: 0.8889\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 4.5325e-04 - accuracy: 1.0000 - val_loss: 0.6426 - val_accuracy: 0.8889\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 4.1491e-04 - accuracy: 1.0000 - val_loss: 0.6414 - val_accuracy: 0.8889\n",
      "Epoch 5\n",
      "max_val :0.8888888955116272\n",
      "save_acc :1.0\n"
     ]
    }
   ],
   "source": [
    "val_acc_list = [];\n",
    "acc_list = [];\n",
    "epochs = 50;\n",
    "batch_size =1 ;\n",
    "kf = KFold(n_splits=5)\n",
    "ep=1;\n",
    "for train_index, test_index in kf.split(X):\n",
    "    x_train, x_test = X[train_index,:], X[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "    img_rows, img_cols = 128, 128\n",
    "\n",
    "    pooling = 'No'\n",
    "    num_sequence = 28;\n",
    "    include_top = False;\n",
    "\n",
    "    input_shape =  [num_sequence,img_rows, img_cols, 3];\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "    inputs = img_input\n",
    "\n",
    "\n",
    "    x = vgg16_head(inputs);\n",
    "\n",
    "    # Create model.\n",
    "    base_model = models.Model(inputs, x, name='vgg16')\n",
    "\n",
    "    weights ='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5';\n",
    "\n",
    "    base_model.load_weights(weights)\n",
    "\n",
    "\n",
    "    head_model = FCHeadNet.build(base_model, 1, 256)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=head_model)\n",
    "\n",
    "    Dont_Want_to_train_all = True;\n",
    "\n",
    "\n",
    "    if (Dont_Want_to_train_all):\n",
    "\n",
    "      for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "    max_val=-1;\n",
    "    save_acc=0;\n",
    "    val_acc=history.history['val_accuracy'];\n",
    "    acc=history.history['accuracy'];\n",
    "    for h in range(len(val_acc)):\n",
    "        if(max_val<=val_acc[h]):\n",
    "            max_val=val_acc[h];\n",
    "            save_acc=acc[h];\n",
    "    print(\"Epoch \"+str(ep))\n",
    "    ep=ep+1;\n",
    "    print(\"max_val :\"+str(max_val))\n",
    "    print(\"save_acc :\"+str(save_acc))\n",
    "    val_acc_list.append(max_val)\n",
    "    acc_list.append(save_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'fold')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc_list,'go-')\n",
    "plt.plot(acc_list,'bo-')\n",
    "plt.title('Model Accuracy 5-kold Crossvalidation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train accuracy 100.0%\n",
      "Average Validation accuracy 86.84210419654846%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Train accuracy \"+str(np.mean(acc_list)*100)+'%')\n",
    "\n",
    "print(\"Average Validation accuracy \"+str(np.mean(val_acc_list)*100)+'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8421052694320679,\n",
       " 0.8333333134651184,\n",
       " 0.8333333134651184,\n",
       " 0.9444444179534912,\n",
       " 0.8888888955116272]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0428 14:31:47.470187 140344891041600 deprecation_wrapper.py:119] From /home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72 samples, validate on 19 samples\n",
      "Epoch 1/50\n",
      "72/72 [==============================] - 23s 315ms/step - loss: 0.7589 - accuracy: 0.6111 - val_loss: 1.2177 - val_accuracy: 0.4211\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 19s 261ms/step - loss: 0.7146 - accuracy: 0.6528 - val_loss: 0.7155 - val_accuracy: 0.4737\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 19s 260ms/step - loss: 0.6279 - accuracy: 0.6667 - val_loss: 0.6268 - val_accuracy: 0.6316\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 18s 248ms/step - loss: 0.5918 - accuracy: 0.6528 - val_loss: 0.7984 - val_accuracy: 0.4737\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 17s 232ms/step - loss: 0.5712 - accuracy: 0.7500 - val_loss: 0.9510 - val_accuracy: 0.4211\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 0.5003 - accuracy: 0.7917 - val_loss: 1.1114 - val_accuracy: 0.4737\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 16s 221ms/step - loss: 0.4979 - accuracy: 0.7778 - val_loss: 1.6039 - val_accuracy: 0.4211\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 16s 221ms/step - loss: 0.3725 - accuracy: 0.8333 - val_loss: 1.2961 - val_accuracy: 0.4211\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 16s 221ms/step - loss: 0.3576 - accuracy: 0.8333 - val_loss: 1.0395 - val_accuracy: 0.4737\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 18s 255ms/step - loss: 0.2849 - accuracy: 0.8889 - val_loss: 1.2964 - val_accuracy: 0.4737\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 16s 221ms/step - loss: 0.2018 - accuracy: 0.9306 - val_loss: 1.8770 - val_accuracy: 0.4211\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 16s 221ms/step - loss: 0.1218 - accuracy: 0.9722 - val_loss: 1.4950 - val_accuracy: 0.4737\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 19s 267ms/step - loss: 0.1326 - accuracy: 0.9583 - val_loss: 2.6963 - val_accuracy: 0.4211\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 16s 221ms/step - loss: 0.0753 - accuracy: 0.9861 - val_loss: 2.3202 - val_accuracy: 0.4211\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 15s 211ms/step - loss: 0.0482 - accuracy: 1.0000 - val_loss: 2.4031 - val_accuracy: 0.4737\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 16s 221ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 2.2688 - val_accuracy: 0.4737\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 16s 220ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 2.6384 - val_accuracy: 0.4737\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 16s 220ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 2.7365 - val_accuracy: 0.4737\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 16s 220ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 3.1013 - val_accuracy: 0.4737\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 17s 237ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 2.5685 - val_accuracy: 0.4737\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 15s 210ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 3.4119 - val_accuracy: 0.4737\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 15s 210ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.5135 - val_accuracy: 0.4737\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 15s 209ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 3.3626 - val_accuracy: 0.4737\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 15s 209ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.4800 - val_accuracy: 0.4737\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 15s 208ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.5597 - val_accuracy: 0.4737\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 15s 209ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.4871 - val_accuracy: 0.4737\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.7447 - val_accuracy: 0.4737\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.6538 - val_accuracy: 0.4737\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.6749 - val_accuracy: 0.4737\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.0634 - val_accuracy: 0.4737\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 16s 224ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.7831 - val_accuracy: 0.4737\n",
      "Epoch 32/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.8939 - val_accuracy: 0.4737\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 16s 222ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.9635 - val_accuracy: 0.4737\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 16s 222ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.9288 - val_accuracy: 0.4737\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0188 - val_accuracy: 0.4737\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.0998 - val_accuracy: 0.4737\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 16s 224ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0762 - val_accuracy: 0.4737\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 16s 222ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.2328 - val_accuracy: 0.4737\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.3179 - val_accuracy: 0.4737\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 9.8091e-04 - accuracy: 1.0000 - val_loss: 4.2205 - val_accuracy: 0.4737\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 16s 222ms/step - loss: 9.2692e-04 - accuracy: 1.0000 - val_loss: 4.2765 - val_accuracy: 0.4737\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 16s 225ms/step - loss: 8.3063e-04 - accuracy: 1.0000 - val_loss: 4.3987 - val_accuracy: 0.4737\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 8.3398e-04 - accuracy: 1.0000 - val_loss: 4.3893 - val_accuracy: 0.4737\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 16s 223ms/step - loss: 7.4016e-04 - accuracy: 1.0000 - val_loss: 4.3387 - val_accuracy: 0.4737\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 16s 222ms/step - loss: 6.8092e-04 - accuracy: 1.0000 - val_loss: 4.4394 - val_accuracy: 0.4737\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 16s 222ms/step - loss: 6.3715e-04 - accuracy: 1.0000 - val_loss: 4.5962 - val_accuracy: 0.4737\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 16s 224ms/step - loss: 6.1336e-04 - accuracy: 1.0000 - val_loss: 4.3824 - val_accuracy: 0.4737\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 16s 222ms/step - loss: 5.5849e-04 - accuracy: 1.0000 - val_loss: 4.6540 - val_accuracy: 0.4737\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 16s 227ms/step - loss: 5.4880e-04 - accuracy: 1.0000 - val_loss: 4.5676 - val_accuracy: 0.4737\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 16s 225ms/step - loss: 5.0502e-04 - accuracy: 1.0000 - val_loss: 4.5722 - val_accuracy: 0.4737\n",
      "Epoch 1\n",
      "max_val :0.6315789222717285\n",
      "save_acc :0.6666667\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 16s 222ms/step - loss: 0.7503 - accuracy: 0.4658 - val_loss: 0.6630 - val_accuracy: 0.6667\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 16s 219ms/step - loss: 0.7266 - accuracy: 0.5616 - val_loss: 0.7376 - val_accuracy: 0.4444\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 16s 214ms/step - loss: 0.6076 - accuracy: 0.6712 - val_loss: 0.7315 - val_accuracy: 0.6111\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 15s 206ms/step - loss: 0.5722 - accuracy: 0.7397 - val_loss: 0.6704 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "73/73 [==============================] - 15s 212ms/step - loss: 0.5284 - accuracy: 0.7260 - val_loss: 0.8494 - val_accuracy: 0.6111\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 16s 214ms/step - loss: 0.4987 - accuracy: 0.7397 - val_loss: 0.8904 - val_accuracy: 0.6111\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 16s 220ms/step - loss: 0.4570 - accuracy: 0.7671 - val_loss: 0.6277 - val_accuracy: 0.6667\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 16s 219ms/step - loss: 0.3836 - accuracy: 0.8356 - val_loss: 0.7025 - val_accuracy: 0.6667\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 16s 220ms/step - loss: 0.3399 - accuracy: 0.8082 - val_loss: 0.6298 - val_accuracy: 0.6667\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 16s 219ms/step - loss: 0.3465 - accuracy: 0.8082 - val_loss: 0.9966 - val_accuracy: 0.6111\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 16s 219ms/step - loss: 0.1877 - accuracy: 0.9589 - val_loss: 0.6821 - val_accuracy: 0.7778\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 16s 219ms/step - loss: 0.1679 - accuracy: 0.9863 - val_loss: 1.2099 - val_accuracy: 0.6111\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 16s 219ms/step - loss: 0.1109 - accuracy: 0.9863 - val_loss: 0.8248 - val_accuracy: 0.7222\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 16s 219ms/step - loss: 0.0680 - accuracy: 1.0000 - val_loss: 1.0792 - val_accuracy: 0.6667\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0488 - accuracy: 1.0000 - val_loss: 0.8318 - val_accuracy: 0.7222\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 18s 244ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 1.0126 - val_accuracy: 0.7222\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 1.6650 - val_accuracy: 0.6111\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 1.3568 - val_accuracy: 0.6111\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 1.3908 - val_accuracy: 0.6111\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 1.3392 - val_accuracy: 0.6667\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 1.4617 - val_accuracy: 0.6667\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.2012 - val_accuracy: 0.7222\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 16s 222ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.4918 - val_accuracy: 0.6667\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.2769 - val_accuracy: 0.6667\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.4625 - val_accuracy: 0.6667\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.4250 - val_accuracy: 0.6667\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.6023 - val_accuracy: 0.6667\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4397 - val_accuracy: 0.6667\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.5719 - val_accuracy: 0.6667\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.8193 - val_accuracy: 0.6111\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.6195 - val_accuracy: 0.6667\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.6558 - val_accuracy: 0.6667\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.7816 - val_accuracy: 0.6667\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.7131 - val_accuracy: 0.6667\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.8317 - val_accuracy: 0.6667\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.6591 - val_accuracy: 0.6667\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.7237 - val_accuracy: 0.6667\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.8790 - val_accuracy: 0.6667\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 17s 229ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.8855 - val_accuracy: 0.6667\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 18s 247ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.8612 - val_accuracy: 0.6667\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 18s 249ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.9678 - val_accuracy: 0.6667\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 18s 253ms/step - loss: 9.7240e-04 - accuracy: 1.0000 - val_loss: 1.9646 - val_accuracy: 0.6667\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 19s 255ms/step - loss: 9.0154e-04 - accuracy: 1.0000 - val_loss: 1.9302 - val_accuracy: 0.6667\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 19s 257ms/step - loss: 8.4925e-04 - accuracy: 1.0000 - val_loss: 1.9050 - val_accuracy: 0.6667\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 18s 249ms/step - loss: 8.3964e-04 - accuracy: 1.0000 - val_loss: 1.8873 - val_accuracy: 0.6667\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 18s 252ms/step - loss: 7.5740e-04 - accuracy: 1.0000 - val_loss: 2.0407 - val_accuracy: 0.6667\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 17s 236ms/step - loss: 6.8209e-04 - accuracy: 1.0000 - val_loss: 1.9816 - val_accuracy: 0.6667\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 6.4627e-04 - accuracy: 1.0000 - val_loss: 1.9874 - val_accuracy: 0.6667\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 5.8824e-04 - accuracy: 1.0000 - val_loss: 1.9959 - val_accuracy: 0.6667\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 17s 234ms/step - loss: 5.5213e-04 - accuracy: 1.0000 - val_loss: 2.1223 - val_accuracy: 0.6667\n",
      "Epoch 2\n",
      "max_val :0.7777777910232544\n",
      "save_acc :0.9589041\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 21s 294ms/step - loss: 0.8241 - accuracy: 0.6438 - val_loss: 0.9287 - val_accuracy: 0.2778\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 21s 284ms/step - loss: 0.6678 - accuracy: 0.5753 - val_loss: 0.5703 - val_accuracy: 0.7222\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 19s 261ms/step - loss: 0.6241 - accuracy: 0.6164 - val_loss: 0.5336 - val_accuracy: 0.7222\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 19s 255ms/step - loss: 0.5530 - accuracy: 0.7260 - val_loss: 0.5368 - val_accuracy: 0.7222\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 18s 246ms/step - loss: 0.5589 - accuracy: 0.7397 - val_loss: 0.5321 - val_accuracy: 0.7778\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 18s 248ms/step - loss: 0.4605 - accuracy: 0.8356 - val_loss: 0.6128 - val_accuracy: 0.7222\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 18s 244ms/step - loss: 0.4587 - accuracy: 0.8082 - val_loss: 0.5537 - val_accuracy: 0.7222\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 21s 289ms/step - loss: 0.4298 - accuracy: 0.8219 - val_loss: 0.5175 - val_accuracy: 0.7222\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 21s 285ms/step - loss: 0.2999 - accuracy: 0.8904 - val_loss: 0.4504 - val_accuracy: 0.7778\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 21s 291ms/step - loss: 0.2644 - accuracy: 0.9041 - val_loss: 0.7437 - val_accuracy: 0.7222\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 21s 281ms/step - loss: 0.2223 - accuracy: 0.9178 - val_loss: 0.5939 - val_accuracy: 0.7222\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 21s 290ms/step - loss: 0.1212 - accuracy: 0.9863 - val_loss: 0.5404 - val_accuracy: 0.7222\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 21s 286ms/step - loss: 0.0851 - accuracy: 1.0000 - val_loss: 0.6476 - val_accuracy: 0.7222\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 19s 263ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 0.6550 - val_accuracy: 0.7222\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 18s 249ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.6677 - val_accuracy: 0.7222\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 18s 251ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 0.5372 - val_accuracy: 0.7778\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 18s 242ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.9177 - val_accuracy: 0.7222\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.7164 - val_accuracy: 0.7222\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 18s 249ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.7989 - val_accuracy: 0.7222\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 18s 252ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.9221 - val_accuracy: 0.7222\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 18s 246ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.9624 - val_accuracy: 0.7222\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.9296 - val_accuracy: 0.7222\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 18s 247ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.9735 - val_accuracy: 0.7222\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 18s 252ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.8418 - val_accuracy: 0.6667\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 18s 245ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.9439 - val_accuracy: 0.7222\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 19s 258ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.0845 - val_accuracy: 0.7222\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 18s 245ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.0545 - val_accuracy: 0.7222\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 18s 246ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.9617 - val_accuracy: 0.7222\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 17s 238ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.1412 - val_accuracy: 0.7222\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 19s 255ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.0504 - val_accuracy: 0.7222\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 18s 242ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.0654 - val_accuracy: 0.7222\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 18s 243ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.1981 - val_accuracy: 0.7222\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 20s 281ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.1697 - val_accuracy: 0.7222\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 18s 249ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.1827 - val_accuracy: 0.7222\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 18s 242ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.1365 - val_accuracy: 0.7222\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 20s 274ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.2936 - val_accuracy: 0.7222\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 18s 243ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1702 - val_accuracy: 0.7222\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 19s 257ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.2083 - val_accuracy: 0.7222\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 17s 236ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.1975 - val_accuracy: 0.7222\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 18s 244ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.2113 - val_accuracy: 0.7222\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 17s 235ms/step - loss: 9.9157e-04 - accuracy: 1.0000 - val_loss: 1.1373 - val_accuracy: 0.6667\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 18s 243ms/step - loss: 9.5091e-04 - accuracy: 1.0000 - val_loss: 1.2880 - val_accuracy: 0.7222\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 17s 232ms/step - loss: 8.3711e-04 - accuracy: 1.0000 - val_loss: 1.2412 - val_accuracy: 0.7222\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 17s 240ms/step - loss: 7.8338e-04 - accuracy: 1.0000 - val_loss: 1.2573 - val_accuracy: 0.7222\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 17s 237ms/step - loss: 7.2164e-04 - accuracy: 1.0000 - val_loss: 1.2412 - val_accuracy: 0.7222\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 17s 238ms/step - loss: 6.7477e-04 - accuracy: 1.0000 - val_loss: 1.2691 - val_accuracy: 0.7222\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 18s 251ms/step - loss: 6.7096e-04 - accuracy: 1.0000 - val_loss: 1.3196 - val_accuracy: 0.7222\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 18s 248ms/step - loss: 6.0083e-04 - accuracy: 1.0000 - val_loss: 1.3417 - val_accuracy: 0.7222\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 17s 236ms/step - loss: 5.6316e-04 - accuracy: 1.0000 - val_loss: 1.2648 - val_accuracy: 0.7222\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 18s 240ms/step - loss: 5.2891e-04 - accuracy: 1.0000 - val_loss: 1.3164 - val_accuracy: 0.7222\n",
      "Epoch 3\n",
      "max_val :0.7777777910232544\n",
      "save_acc :1.0\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 18s 248ms/step - loss: 0.7758 - accuracy: 0.5342 - val_loss: 0.6920 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 17s 239ms/step - loss: 0.6276 - accuracy: 0.6438 - val_loss: 0.9356 - val_accuracy: 0.5556\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 18s 240ms/step - loss: 0.5866 - accuracy: 0.7123 - val_loss: 0.6694 - val_accuracy: 0.5556\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 18s 242ms/step - loss: 0.5324 - accuracy: 0.7397 - val_loss: 1.3261 - val_accuracy: 0.5556\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 18s 241ms/step - loss: 0.4648 - accuracy: 0.7671 - val_loss: 0.9934 - val_accuracy: 0.5556\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 17s 239ms/step - loss: 0.4552 - accuracy: 0.6986 - val_loss: 0.8325 - val_accuracy: 0.5556\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 17s 239ms/step - loss: 0.3283 - accuracy: 0.8767 - val_loss: 0.6598 - val_accuracy: 0.6111\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 18s 243ms/step - loss: 0.3005 - accuracy: 0.8219 - val_loss: 0.9152 - val_accuracy: 0.6111\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 17s 239ms/step - loss: 0.3060 - accuracy: 0.8493 - val_loss: 1.3472 - val_accuracy: 0.5556\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 17s 239ms/step - loss: 0.2240 - accuracy: 0.9041 - val_loss: 0.8390 - val_accuracy: 0.6111\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 17s 239ms/step - loss: 0.1550 - accuracy: 0.9726 - val_loss: 1.2315 - val_accuracy: 0.5556\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 17s 239ms/step - loss: 0.1233 - accuracy: 0.9726 - val_loss: 1.4643 - val_accuracy: 0.5556\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 19s 255ms/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 1.2253 - val_accuracy: 0.5556\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 1.6572 - val_accuracy: 0.5556\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0434 - accuracy: 0.9863 - val_loss: 2.1508 - val_accuracy: 0.5556\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 2.4547 - val_accuracy: 0.5556\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0411 - accuracy: 0.9863 - val_loss: 2.4886 - val_accuracy: 0.5556\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 1.6386 - val_accuracy: 0.6111\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 19s 258ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.5795 - val_accuracy: 0.5556\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 19s 257ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 2.5320 - val_accuracy: 0.5556\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 19s 255ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.5811 - val_accuracy: 0.5556\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 19s 260ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.2856 - val_accuracy: 0.5556\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 19s 255ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.3929 - val_accuracy: 0.5556\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 19s 257ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.6447 - val_accuracy: 0.5556\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 19s 255ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.5748 - val_accuracy: 0.5556\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 19s 255ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.7352 - val_accuracy: 0.5556\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.7765 - val_accuracy: 0.5556\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 19s 255ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.1495 - val_accuracy: 0.5556\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 19s 256ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.8800 - val_accuracy: 0.5556\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 19s 255ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.0199 - val_accuracy: 0.5556\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 18s 244ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.0808 - val_accuracy: 0.5556\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 18s 244ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.1164 - val_accuracy: 0.5556\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.0228 - val_accuracy: 0.5556\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 19s 259ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9761 - val_accuracy: 0.5556\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.3781 - val_accuracy: 0.5556\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.3178 - val_accuracy: 0.5556\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.1963 - val_accuracy: 0.5556\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 9.7874e-04 - accuracy: 1.0000 - val_loss: 3.4427 - val_accuracy: 0.5556\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 18s 252ms/step - loss: 9.4011e-04 - accuracy: 1.0000 - val_loss: 3.4278 - val_accuracy: 0.5556\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 8.0172e-04 - accuracy: 1.0000 - val_loss: 3.2325 - val_accuracy: 0.5556\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 18s 246ms/step - loss: 7.7106e-04 - accuracy: 1.0000 - val_loss: 3.2619 - val_accuracy: 0.5556\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 18s 245ms/step - loss: 7.0766e-04 - accuracy: 1.0000 - val_loss: 3.3239 - val_accuracy: 0.5556\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 18s 253ms/step - loss: 7.7305e-04 - accuracy: 1.0000 - val_loss: 3.4978 - val_accuracy: 0.5556\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 18s 252ms/step - loss: 5.7099e-04 - accuracy: 1.0000 - val_loss: 3.3648 - val_accuracy: 0.5556\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 17s 239ms/step - loss: 5.5918e-04 - accuracy: 1.0000 - val_loss: 3.4944 - val_accuracy: 0.5556\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 17s 235ms/step - loss: 5.2080e-04 - accuracy: 1.0000 - val_loss: 3.4519 - val_accuracy: 0.5556\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 17s 235ms/step - loss: 4.9805e-04 - accuracy: 1.0000 - val_loss: 3.4904 - val_accuracy: 0.5556\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 17s 237ms/step - loss: 4.6545e-04 - accuracy: 1.0000 - val_loss: 3.5322 - val_accuracy: 0.5556\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 17s 237ms/step - loss: 4.4715e-04 - accuracy: 1.0000 - val_loss: 3.6300 - val_accuracy: 0.5556\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 17s 234ms/step - loss: 4.3065e-04 - accuracy: 1.0000 - val_loss: 3.5793 - val_accuracy: 0.5556\n",
      "Epoch 4\n",
      "max_val :0.6111111044883728\n",
      "save_acc :1.0\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 18s 243ms/step - loss: 0.7561 - accuracy: 0.5753 - val_loss: 0.5787 - val_accuracy: 0.7222\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 17s 239ms/step - loss: 0.6337 - accuracy: 0.6575 - val_loss: 0.7264 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 18s 241ms/step - loss: 0.6470 - accuracy: 0.6164 - val_loss: 0.6083 - val_accuracy: 0.7222\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 17s 235ms/step - loss: 0.6360 - accuracy: 0.6027 - val_loss: 0.5486 - val_accuracy: 0.7222\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 17s 236ms/step - loss: 0.4978 - accuracy: 0.7808 - val_loss: 0.6590 - val_accuracy: 0.7222\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 18s 246ms/step - loss: 0.4931 - accuracy: 0.7534 - val_loss: 0.5718 - val_accuracy: 0.7222\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 19s 256ms/step - loss: 0.3863 - accuracy: 0.8493 - val_loss: 0.5511 - val_accuracy: 0.7222\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 18s 251ms/step - loss: 0.3794 - accuracy: 0.8493 - val_loss: 0.5707 - val_accuracy: 0.6667\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 18s 253ms/step - loss: 0.2935 - accuracy: 0.8904 - val_loss: 0.7626 - val_accuracy: 0.7222\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 19s 266ms/step - loss: 0.1872 - accuracy: 0.9452 - val_loss: 0.5863 - val_accuracy: 0.7222\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 20s 275ms/step - loss: 0.1461 - accuracy: 0.9863 - val_loss: 0.9592 - val_accuracy: 0.7222\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 20s 280ms/step - loss: 0.1732 - accuracy: 0.9178 - val_loss: 0.6744 - val_accuracy: 0.7222\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 19s 266ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 0.8090 - val_accuracy: 0.6667\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 20s 272ms/step - loss: 0.0524 - accuracy: 1.0000 - val_loss: 1.1484 - val_accuracy: 0.7222\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 20s 272ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 1.0843 - val_accuracy: 0.7222\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 20s 273ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 1.0704 - val_accuracy: 0.6667\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 20s 275ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 1.2710 - val_accuracy: 0.7222\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 20s 274ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 1.1703 - val_accuracy: 0.6667\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 20s 277ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 1.2040 - val_accuracy: 0.7222\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 20s 278ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.0960 - val_accuracy: 0.7222\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 21s 281ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.2315 - val_accuracy: 0.7222\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 20s 276ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.3530 - val_accuracy: 0.7222\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 20s 279ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2568 - val_accuracy: 0.7222\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 20s 279ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.3184 - val_accuracy: 0.7222\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 20s 280ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3081 - val_accuracy: 0.7222\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 20s 280ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.4436 - val_accuracy: 0.7222\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 20s 279ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.4611 - val_accuracy: 0.7222\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 20s 274ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.4563 - val_accuracy: 0.7222\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 20s 274ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.4615 - val_accuracy: 0.7222\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 19s 264ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.4687 - val_accuracy: 0.7222\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 18s 249ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.5324 - val_accuracy: 0.7222\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 18s 246ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.5367 - val_accuracy: 0.7222\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 20s 268ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.5887 - val_accuracy: 0.7222\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 19s 255ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.5754 - val_accuracy: 0.7222\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 18s 246ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.6088 - val_accuracy: 0.7222\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 18s 249ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.6357 - val_accuracy: 0.7222\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 19s 264ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.6443 - val_accuracy: 0.7222\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 18s 243ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.6010 - val_accuracy: 0.7222\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 18s 245ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.6638 - val_accuracy: 0.7222\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 19s 261ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.6937 - val_accuracy: 0.7222\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 20s 273ms/step - loss: 9.9090e-04 - accuracy: 1.0000 - val_loss: 1.6561 - val_accuracy: 0.7222\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 18s 247ms/step - loss: 9.3663e-04 - accuracy: 1.0000 - val_loss: 1.7005 - val_accuracy: 0.7222\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 20s 272ms/step - loss: 8.3082e-04 - accuracy: 1.0000 - val_loss: 1.7059 - val_accuracy: 0.7222\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 20s 274ms/step - loss: 8.2282e-04 - accuracy: 1.0000 - val_loss: 1.7187 - val_accuracy: 0.7222\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 20s 271ms/step - loss: 7.7290e-04 - accuracy: 1.0000 - val_loss: 1.7502 - val_accuracy: 0.7222\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 19s 256ms/step - loss: 7.1663e-04 - accuracy: 1.0000 - val_loss: 1.7137 - val_accuracy: 0.7222\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 18s 250ms/step - loss: 6.5111e-04 - accuracy: 1.0000 - val_loss: 1.7618 - val_accuracy: 0.7222\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 18s 251ms/step - loss: 6.2229e-04 - accuracy: 1.0000 - val_loss: 1.8147 - val_accuracy: 0.7222\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 19s 254ms/step - loss: 6.2003e-04 - accuracy: 1.0000 - val_loss: 1.7821 - val_accuracy: 0.7222\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 18s 249ms/step - loss: 5.3955e-04 - accuracy: 1.0000 - val_loss: 1.8084 - val_accuracy: 0.7222\n",
      "Epoch 5\n",
      "max_val :0.7222222089767456\n",
      "save_acc :1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "val_acc_list = [];\n",
    "acc_list = [];\n",
    "epochs = 50;\n",
    "batch_size =1 ;\n",
    "kf = KFold(n_splits=5)\n",
    "ep=1;\n",
    "for train_index, test_index in kf.split(X):\n",
    "    x_train, x_test = X[train_index,:], X[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    x_test_mask = (x_test[...,0]>0.2) | (x_test[...,1]>0.2) | (x_test[...,2]>0.2)\n",
    "    x_test_mask  = x_test_mask*1;\n",
    "    x_test_mask = x_test_mask[...,None]\n",
    "    x_test_mask =np.concatenate([x_test_mask,x_test_mask,x_test_mask],axis=-1)\n",
    "\n",
    "    img_rows, img_cols = 128, 128\n",
    "\n",
    "    pooling = 'No'\n",
    "    num_sequence = 28;\n",
    "    include_top = False;\n",
    "\n",
    "    input_shape =  [num_sequence,img_rows, img_cols, 3];\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "    inputs = img_input\n",
    "\n",
    "\n",
    "    x = vgg16_head(inputs);\n",
    "\n",
    "    # Create model.\n",
    "    base_model = models.Model(inputs, x, name='vgg16')\n",
    "\n",
    "    weights ='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5';\n",
    "\n",
    "    base_model.load_weights(weights)\n",
    "\n",
    "\n",
    "    head_model = FCHeadNet.build(base_model, 1, 256)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=head_model)\n",
    "\n",
    "    Dont_Want_to_train_all = True;\n",
    "\n",
    "\n",
    "    if (Dont_Want_to_train_all):\n",
    "\n",
    "      for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test_mask, y_test))\n",
    "\n",
    "\n",
    "    max_val=-1;\n",
    "    save_acc=0;\n",
    "    val_acc=history.history['val_accuracy'];\n",
    "    acc=history.history['accuracy'];\n",
    "    for h in range(len(val_acc)):\n",
    "        if(max_val<=val_acc[h]):\n",
    "            max_val=val_acc[h];\n",
    "            save_acc=acc[h];\n",
    "    print(\"Epoch \"+str(ep))\n",
    "    ep=ep+1;\n",
    "    print(\"max_val :\"+str(max_val))\n",
    "    print(\"save_acc :\"+str(save_acc))\n",
    "    val_acc_list.append(max_val)\n",
    "    acc_list.append(save_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'fold')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc_list,'go-')\n",
    "plt.plot(acc_list,'bo-')\n",
    "plt.title('Model Accuracy 5-kold Crossvalidation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train accuracy 92.51141548156738%\n",
      "Average Validation accuracy 70.40935635566711%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Train accuracy \"+str(np.mean(acc_list)*100)+'%')\n",
    "\n",
    "print(\"Average Validation accuracy \"+str(np.mean(val_acc_list)*100)+'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?, 224, 224, 3)\n",
    "self.input_shape_r\n",
    "(None, 28, 256)\n",
    "inputs\n",
    "(?, 28, 1, 256)\n",
    "dot product\n",
    "(?, 28, 1, 256)\n",
    "tanh\n",
    "(?, 28, 1, 256)\n",
    "combUV\n",
    "(?, 28, 1, 256)\n",
    "attention_weights\n",
    "(?, 28, 1, 1)\n",
    "attention_weights sum\n",
    "(?, 28, 1, 1)\n",
    "attention_weights last\n",
    "(?, 28, 1)\n",
    "output\n",
    "(?, 256)\n",
    "\n",
    "W0428 14:31:47.470187 140344891041600 deprecation_wrapper.py:119] From /home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
    "\n",
    "Train on 72 samples, validate on 19 samples\n",
    "Epoch 1/50\n",
    "72/72 [==============================] - 23s 315ms/step - loss: 0.7589 - accuracy: 0.6111 - val_loss: 1.2177 - val_accuracy: 0.4211\n",
    "Epoch 2/50\n",
    "72/72 [==============================] - 19s 261ms/step - loss: 0.7146 - accuracy: 0.6528 - val_loss: 0.7155 - val_accuracy: 0.4737\n",
    "Epoch 3/50\n",
    "72/72 [==============================] - 19s 260ms/step - loss: 0.6279 - accuracy: 0.6667 - val_loss: 0.6268 - val_accuracy: 0.6316\n",
    "Epoch 4/50\n",
    "72/72 [==============================] - 18s 248ms/step - loss: 0.5918 - accuracy: 0.6528 - val_loss: 0.7984 - val_accuracy: 0.4737\n",
    "Epoch 5/50\n",
    "72/72 [==============================] - 17s 232ms/step - loss: 0.5712 - accuracy: 0.7500 - val_loss: 0.9510 - val_accuracy: 0.4211\n",
    "Epoch 6/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 0.5003 - accuracy: 0.7917 - val_loss: 1.1114 - val_accuracy: 0.4737\n",
    "Epoch 7/50\n",
    "72/72 [==============================] - 16s 221ms/step - loss: 0.4979 - accuracy: 0.7778 - val_loss: 1.6039 - val_accuracy: 0.4211\n",
    "Epoch 8/50\n",
    "72/72 [==============================] - 16s 221ms/step - loss: 0.3725 - accuracy: 0.8333 - val_loss: 1.2961 - val_accuracy: 0.4211\n",
    "Epoch 9/50\n",
    "72/72 [==============================] - 16s 221ms/step - loss: 0.3576 - accuracy: 0.8333 - val_loss: 1.0395 - val_accuracy: 0.4737\n",
    "Epoch 10/50\n",
    "72/72 [==============================] - 18s 255ms/step - loss: 0.2849 - accuracy: 0.8889 - val_loss: 1.2964 - val_accuracy: 0.4737\n",
    "Epoch 11/50\n",
    "72/72 [==============================] - 16s 221ms/step - loss: 0.2018 - accuracy: 0.9306 - val_loss: 1.8770 - val_accuracy: 0.4211\n",
    "Epoch 12/50\n",
    "72/72 [==============================] - 16s 221ms/step - loss: 0.1218 - accuracy: 0.9722 - val_loss: 1.4950 - val_accuracy: 0.4737\n",
    "Epoch 13/50\n",
    "72/72 [==============================] - 19s 267ms/step - loss: 0.1326 - accuracy: 0.9583 - val_loss: 2.6963 - val_accuracy: 0.4211\n",
    "Epoch 14/50\n",
    "72/72 [==============================] - 16s 221ms/step - loss: 0.0753 - accuracy: 0.9861 - val_loss: 2.3202 - val_accuracy: 0.4211\n",
    "Epoch 15/50\n",
    "72/72 [==============================] - 15s 211ms/step - loss: 0.0482 - accuracy: 1.0000 - val_loss: 2.4031 - val_accuracy: 0.4737\n",
    "Epoch 16/50\n",
    "72/72 [==============================] - 16s 221ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 2.2688 - val_accuracy: 0.4737\n",
    "Epoch 17/50\n",
    "72/72 [==============================] - 16s 220ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 2.6384 - val_accuracy: 0.4737\n",
    "Epoch 18/50\n",
    "72/72 [==============================] - 16s 220ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 2.7365 - val_accuracy: 0.4737\n",
    "Epoch 19/50\n",
    "72/72 [==============================] - 16s 220ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 3.1013 - val_accuracy: 0.4737\n",
    "Epoch 20/50\n",
    "72/72 [==============================] - 17s 237ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 2.5685 - val_accuracy: 0.4737\n",
    "Epoch 21/50\n",
    "72/72 [==============================] - 15s 210ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 3.4119 - val_accuracy: 0.4737\n",
    "Epoch 22/50\n",
    "72/72 [==============================] - 15s 210ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.5135 - val_accuracy: 0.4737\n",
    "Epoch 23/50\n",
    "72/72 [==============================] - 15s 209ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 3.3626 - val_accuracy: 0.4737\n",
    "Epoch 24/50\n",
    "72/72 [==============================] - 15s 209ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.4800 - val_accuracy: 0.4737\n",
    "Epoch 25/50\n",
    "72/72 [==============================] - 15s 208ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.5597 - val_accuracy: 0.4737\n",
    "Epoch 26/50\n",
    "72/72 [==============================] - 15s 209ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.4871 - val_accuracy: 0.4737\n",
    "Epoch 27/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.7447 - val_accuracy: 0.4737\n",
    "Epoch 28/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.6538 - val_accuracy: 0.4737\n",
    "Epoch 29/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.6749 - val_accuracy: 0.4737\n",
    "Epoch 30/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.0634 - val_accuracy: 0.4737\n",
    "Epoch 31/50\n",
    "72/72 [==============================] - 16s 224ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.7831 - val_accuracy: 0.4737\n",
    "Epoch 32/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.8939 - val_accuracy: 0.4737\n",
    "Epoch 33/50\n",
    "72/72 [==============================] - 16s 222ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.9635 - val_accuracy: 0.4737\n",
    "Epoch 34/50\n",
    "72/72 [==============================] - 16s 222ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.9288 - val_accuracy: 0.4737\n",
    "Epoch 35/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.0188 - val_accuracy: 0.4737\n",
    "Epoch 36/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.0998 - val_accuracy: 0.4737\n",
    "Epoch 37/50\n",
    "72/72 [==============================] - 16s 224ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.0762 - val_accuracy: 0.4737\n",
    "Epoch 38/50\n",
    "72/72 [==============================] - 16s 222ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.2328 - val_accuracy: 0.4737\n",
    "Epoch 39/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.3179 - val_accuracy: 0.4737\n",
    "Epoch 40/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 9.8091e-04 - accuracy: 1.0000 - val_loss: 4.2205 - val_accuracy: 0.4737\n",
    "Epoch 41/50\n",
    "72/72 [==============================] - 16s 222ms/step - loss: 9.2692e-04 - accuracy: 1.0000 - val_loss: 4.2765 - val_accuracy: 0.4737\n",
    "Epoch 42/50\n",
    "72/72 [==============================] - 16s 225ms/step - loss: 8.3063e-04 - accuracy: 1.0000 - val_loss: 4.3987 - val_accuracy: 0.4737\n",
    "Epoch 43/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 8.3398e-04 - accuracy: 1.0000 - val_loss: 4.3893 - val_accuracy: 0.4737\n",
    "Epoch 44/50\n",
    "72/72 [==============================] - 16s 223ms/step - loss: 7.4016e-04 - accuracy: 1.0000 - val_loss: 4.3387 - val_accuracy: 0.4737\n",
    "Epoch 45/50\n",
    "72/72 [==============================] - 16s 222ms/step - loss: 6.8092e-04 - accuracy: 1.0000 - val_loss: 4.4394 - val_accuracy: 0.4737\n",
    "Epoch 46/50\n",
    "72/72 [==============================] - 16s 222ms/step - loss: 6.3715e-04 - accuracy: 1.0000 - val_loss: 4.5962 - val_accuracy: 0.4737\n",
    "Epoch 47/50\n",
    "72/72 [==============================] - 16s 224ms/step - loss: 6.1336e-04 - accuracy: 1.0000 - val_loss: 4.3824 - val_accuracy: 0.4737\n",
    "Epoch 48/50\n",
    "72/72 [==============================] - 16s 222ms/step - loss: 5.5849e-04 - accuracy: 1.0000 - val_loss: 4.6540 - val_accuracy: 0.4737\n",
    "Epoch 49/50\n",
    "72/72 [==============================] - 16s 227ms/step - loss: 5.4880e-04 - accuracy: 1.0000 - val_loss: 4.5676 - val_accuracy: 0.4737\n",
    "Epoch 50/50\n",
    "72/72 [==============================] - 16s 225ms/step - loss: 5.0502e-04 - accuracy: 1.0000 - val_loss: 4.5722 - val_accuracy: 0.4737\n",
    "Epoch 1\n",
    "max_val :0.6315789222717285\n",
    "save_acc :0.6666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_list_correct = val_acc_list\n",
    "acc_list_correct = acc_list;\n",
    "\n",
    "val_acc_list_correct[0] =  0.4737\n",
    "acc_list_correct[0]=1.0000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'fold')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc_list,'go-')\n",
    "plt.plot(acc_list,'bo-')\n",
    "plt.title('Model Accuracy 5-kold Crossvalidation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train accuracy 99.17808175086975%\n",
      "Average Validation accuracy 67.25177791023255%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Train accuracy \"+str(np.mean(acc_list)*100)+'%')\n",
    "\n",
    "print(\"Average Validation accuracy \"+str(np.mean(val_acc_list)*100)+'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4737,\n",
       " 0.7777777910232544,\n",
       " 0.7777777910232544,\n",
       " 0.6111111044883728,\n",
       " 0.7222222089767456]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.9589041, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch 3\n",
    "- accuracy: 0.6667 - val_loss: 0.6268 - val_accuracy: 0.6316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 72 samples, validate on 19 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[28,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node time_distributed_179/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Mean_8/_2813]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[28,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node time_distributed_179/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-2a12e0291637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m               validation_data=(x_test_mask, y_test))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[28,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node time_distributed_179/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Mean_8/_2813]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[28,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node time_distributed_179/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "val_acc_list = [];\n",
    "acc_list = [];\n",
    "epochs = 50;\n",
    "batch_size =1 ;\n",
    "kf = KFold(n_splits=5)\n",
    "ep=1;\n",
    "for train_index, test_index in kf.split(X):\n",
    "    x_train, x_test = X[train_index,:], X[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    x_train_mask = (x_train[...,0]>0.2) | (x_train[...,1]>0.2) | (x_train[...,2]>0.2)\n",
    "    x_train_mask  = x_train_mask*1;\n",
    "    x_train_mask = x_train_mask[...,None]\n",
    "    x_train_mask =np.concatenate([x_train_mask,x_train_mask,x_train_mask],axis=-1)\n",
    "    \n",
    "    \n",
    "    x_test_mask = (x_test[...,0]>0.2) | (x_test[...,1]>0.2) | (x_test[...,2]>0.2)\n",
    "    x_test_mask  = x_test_mask*1;\n",
    "    x_test_mask = x_test_mask[...,None]\n",
    "    x_test_mask =np.concatenate([x_test_mask,x_test_mask,x_test_mask],axis=-1)\n",
    "\n",
    "    img_rows, img_cols = 128, 128\n",
    "\n",
    "    pooling = 'No'\n",
    "    num_sequence = 28;\n",
    "    include_top = False;\n",
    "\n",
    "    input_shape =  [num_sequence,img_rows, img_cols, 3];\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "    inputs = img_input\n",
    "\n",
    "\n",
    "    x = vgg16_head(inputs);\n",
    "\n",
    "    # Create model.\n",
    "    base_model = models.Model(inputs, x, name='vgg16')\n",
    "\n",
    "    weights ='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5';\n",
    "\n",
    "    base_model.load_weights(weights)\n",
    "\n",
    "\n",
    "    head_model = FCHeadNet.build(base_model, 1, 256)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=head_model)\n",
    "\n",
    "    Dont_Want_to_train_all = True;\n",
    "\n",
    "\n",
    "    if (Dont_Want_to_train_all):\n",
    "\n",
    "      for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train_mask, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test_mask, y_test))\n",
    "\n",
    "\n",
    "    max_val=-1;\n",
    "    save_acc=0;\n",
    "    val_acc=history.history['val_accuracy'];\n",
    "    acc=history.history['accuracy'];\n",
    "    for h in range(len(val_acc)):\n",
    "        if(max_val<=val_acc[h]):\n",
    "            max_val=val_acc[h];\n",
    "            save_acc=acc[h];\n",
    "    print(\"Epoch \"+str(ep))\n",
    "    ep=ep+1;\n",
    "    print(\"max_val :\"+str(max_val))\n",
    "    print(\"save_acc :\"+str(save_acc))\n",
    "    val_acc_list.append(max_val)\n",
    "    acc_list.append(save_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
