{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13490629357577604960\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6410313770835342218\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6069456372225395705\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5618597888\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17200216036153251867\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,TimeDistributed\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import layers,models\n",
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import heapq\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScaleLayer(Layer):\n",
    "\n",
    "    def __init__(self,scale, **kwargs):\n",
    "\n",
    "        self.scale=scale;\n",
    "        \n",
    "        super(ScaleLayer, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ScaleLayer, self).build(input_shape)\n",
    "\n",
    "       \n",
    "    def call(self, inputs):\n",
    "\n",
    "        output=tf.image.resize(inputs,[self.scale,self.scale],method=tf.image.ResizeMethod.BICUBIC)\n",
    "\n",
    "\n",
    "        print(output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "       \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],self.scale,self.scale,3)\n",
    "\n",
    "\n",
    "\n",
    "def vgg16_head(img_input):\n",
    "\n",
    "    pooling = 'No'\n",
    "\n",
    "    include_top = False;\n",
    "\n",
    "\n",
    "\n",
    "    x = TimeDistributed(ScaleLayer(224))(img_input)\n",
    "\n",
    "    x = TimeDistributed(layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv2'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = TimeDistributed(layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = TimeDistributed(layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv2'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv3'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv2'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv3'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv1'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv2'))(x)\n",
    "    x = TimeDistributed(layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv3'))(x)\n",
    "    x = TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = layers.Flatten(name='flatten')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "        elif pooling =='No':\n",
    "            x = x;\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        \n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "\n",
    "        self.input_shape_r = input_shape;\n",
    "        print('self.input_shape_r')\n",
    "        print(self.input_shape_r)\n",
    "\n",
    "        L_size = 256;\n",
    "\n",
    "        self.w = self.add_weight(name='w', \n",
    "                              shape=[L_size,1],\n",
    "                              initializer='uniform',\n",
    "                              trainable=True)\n",
    "        self.V = self.add_weight(name='V', \n",
    "                      shape=[input_shape[2],L_size],\n",
    "                      initializer='uniform',\n",
    "                      trainable=True)\n",
    "\n",
    "        self.U = self.add_weight(name='U', \n",
    "                      shape=[input_shape[2],L_size],\n",
    "                      initializer='uniform',\n",
    "                      trainable=True)\n",
    "\n",
    "        \n",
    "\n",
    "        self.bias_w = self.add_weight(shape=[1,1],\n",
    "                              initializer='uniform',\n",
    "                              name='bias_w')\n",
    "        self.bias_V = self.add_weight(shape=[1,L_size],\n",
    "                                      initializer='uniform',\n",
    "                                      name='bias_V')\n",
    "        self.bias_U = self.add_weight(shape=[1,L_size],\n",
    "                                      initializer='uniform',\n",
    "                                      name='bias_U')\n",
    "        \n",
    "        self.Save = self.add_weight(name='ForSave', \n",
    "              shape=[10,10],\n",
    "              initializer='uniform',\n",
    "              trainable=False)\n",
    "      \n",
    "\n",
    "  \n",
    "       \n",
    "    def call(self, inputs):\n",
    "\n",
    "\n",
    "        inputs_r = K.expand_dims(inputs,axis=-2)\n",
    "\n",
    "\n",
    "        print('inputs')\n",
    "        print(inputs_r.shape)\n",
    "\n",
    "\n",
    "        dot_products_V = K.dot(inputs_r,self.V)\n",
    "        dot_products_U = K.dot(inputs_r,self.U)\n",
    "\n",
    "        dot_products_V += self.bias_V;\n",
    "        dot_products_U += self.bias_U;\n",
    "\n",
    "\n",
    "        print('dot product')\n",
    "        print(dot_products_V.shape)\n",
    "\n",
    "        tanh_V = K.tanh(dot_products_V);\n",
    "        sigmoid_U = K.sigmoid(dot_products_U);\n",
    "\n",
    "        print('tanh')\n",
    "        print(tanh_V.shape)\n",
    "\n",
    "        comb_UV = tanh_V*sigmoid_U;\n",
    "\n",
    "        print('combUV')\n",
    "        print(comb_UV.shape)\n",
    "\n",
    "        attention_weights = K.dot(comb_UV,self.w);\n",
    "\n",
    "        attention_weights +=self.bias_w;\n",
    "\n",
    "        attention_weights = K.exp(attention_weights);\n",
    "\n",
    "\n",
    "        print('attention_weights')\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "\n",
    "        attention_weights /= K.cast(K.sum(attention_weights,\n",
    "                                  axis=1,\n",
    "                                  keepdims=True) + K.epsilon(),\n",
    "                            K.floatx());\n",
    "        print('attention_weights sum')\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        #attention_weights = K.expand_dims(attention_weights);\n",
    "        attention_weights= K.squeeze(attention_weights,axis = -1);\n",
    "        print('attention_weights last')\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        weighted_output = inputs * attention_weights;\n",
    "\n",
    "        self.Save =  attention_weights;\n",
    "\n",
    "\n",
    "        output = K.sum(weighted_output, axis=1)\n",
    "\n",
    "        print('output')\n",
    "        print(output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[2])\n",
    "    \n",
    "    \n",
    "class FCHeadNet:\n",
    "  @staticmethod\n",
    "  def build(baseModel, classes, D):\n",
    "    # initialize the head model that will be placed on top of\n",
    "    # the base, then add a FC layer\n",
    "    headModel = baseModel.output\n",
    "    headModel = TimeDistributed(layers.Flatten(name='flatten'))(headModel)\n",
    "    headModel = TimeDistributed(layers.Dense(D, activation='relu', name='fc1'))(headModel)\n",
    "    headModel = TimeDistributed(layers.Dense(D, activation='relu', name='fc2'))(headModel)\n",
    "    headModel = AttentionLayer(name='attentionlayer')(headModel)\n",
    "    headModel = layers.Dense(classes, activation='sigmoid', name='predictions')(headModel)\n",
    "    # add a softmax layer\n",
    "    #headModel = layers.Dense(classes, activation=\"softmax\")(headModel)\n",
    "    #headModel = layers.Dense(classes, activation='softmax', name='predictions2')(headModel)\n",
    "\n",
    "    # return the model\n",
    "    return headModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 28, 128, 128, 3)\n",
      "(91,)\n",
      "[20, 37, 89, 1, 82, 75, 70, 26, 74, 53, 13, 27, 86, 29, 23, 22, 60, 64, 5, 72, 6, 84, 45, 68, 76, 58, 90, 31, 34, 46, 32, 49, 48, 38, 17, 40, 43, 67, 77, 10, 44, 25, 33, 88, 59, 85, 50, 66, 14, 81, 63, 8, 56, 51, 15, 79, 19, 80, 87, 0, 39, 36, 41, 71, 61, 2, 18, 83, 9, 35, 21, 7, 55, 52, 65, 30, 12, 3, 69, 54, 11, 24, 78, 16, 57, 47, 73, 62, 42, 4, 28]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "batch_size = 1\n",
    "num_classes = 2\n",
    "epochs = 100\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 128, 128\n",
    "\n",
    "# the data, split between train and test sets\n",
    "\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "p='/home/peppermint/Data_Sci/Attention_ML/2548_many.h5';\n",
    "\n",
    "db=h5py.File(p);\n",
    "\n",
    "\n",
    "X_stk, y_stk = db['images'],db['labels'];\n",
    "\n",
    "X =np.zeros((91,28,128,128,3))\n",
    "y = np.zeros((91,))\n",
    "\n",
    "for ij in range(91):\n",
    "    X[ij,...] = X_stk[ij*28:(ij+1)*28,...]\n",
    "    y[ij,...] = y_stk[(ij*28+(ij+1)*28)//2]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "total_size = X.shape[0];\n",
    "index_random = np.arange(total_size)\n",
    "i = int(X.shape[0] * 3/4)\n",
    "\n",
    "random.shuffle(index_random)\n",
    "\n",
    "index_random=list(index_random)\n",
    "\n",
    "X = X[list(index_random)]\n",
    "\n",
    "y = y[list(index_random)]\n",
    "\n",
    "\n",
    "\n",
    "print(list(index_random))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0428 00:57:56.250262 139842270504768 deprecation_wrapper.py:119] From /home/peppermint/.virtualenvs/cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72 samples, validate on 19 samples\n",
      "Epoch 1/50\n",
      "72/72 [==============================] - 18s 253ms/step - loss: 0.7844 - accuracy: 0.5972 - val_loss: 0.6899 - val_accuracy: 0.5263\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.6261 - accuracy: 0.6389 - val_loss: 0.7229 - val_accuracy: 0.5263\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.6438 - accuracy: 0.6111 - val_loss: 0.6459 - val_accuracy: 0.6316\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.5708 - accuracy: 0.6806 - val_loss: 0.6051 - val_accuracy: 0.6842\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.5705 - accuracy: 0.7361 - val_loss: 0.5920 - val_accuracy: 0.7368\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.5394 - accuracy: 0.7222 - val_loss: 0.5724 - val_accuracy: 0.7368\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.4759 - accuracy: 0.7778 - val_loss: 0.5826 - val_accuracy: 0.6842\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.4357 - accuracy: 0.8194 - val_loss: 0.6117 - val_accuracy: 0.6842\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 15s 204ms/step - loss: 0.4335 - accuracy: 0.8056 - val_loss: 0.5185 - val_accuracy: 0.7368\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.2883 - accuracy: 0.9028 - val_loss: 0.5785 - val_accuracy: 0.7368\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.2476 - accuracy: 0.9028 - val_loss: 0.5601 - val_accuracy: 0.7368\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.1840 - accuracy: 0.9444 - val_loss: 0.5370 - val_accuracy: 0.8421\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.1571 - accuracy: 0.9444 - val_loss: 0.6923 - val_accuracy: 0.7368\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.1660 - accuracy: 0.9583 - val_loss: 0.9298 - val_accuracy: 0.6316\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0834 - accuracy: 0.9861 - val_loss: 0.4479 - val_accuracy: 0.7368\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 15s 207ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.4119 - val_accuracy: 0.8421\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 0.7368\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.4564 - val_accuracy: 0.8421\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.6897 - val_accuracy: 0.7895\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 0.7895\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.4801 - val_accuracy: 0.8421\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3983 - val_accuracy: 0.8421\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 15s 205ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6666 - val_accuracy: 0.7895\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.8421\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.4859 - val_accuracy: 0.8421\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 15s 206ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.5573 - val_accuracy: 0.8421\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5543 - val_accuracy: 0.8421\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5630 - val_accuracy: 0.8421\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4674 - val_accuracy: 0.8421\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5089 - val_accuracy: 0.8421\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5784 - val_accuracy: 0.8421\n",
      "Epoch 32/50\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5292 - val_accuracy: 0.8421\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 15s 201ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5482 - val_accuracy: 0.8421\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.7004 - val_accuracy: 0.7895\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5858 - val_accuracy: 0.8421\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5835 - val_accuracy: 0.8421\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6156 - val_accuracy: 0.8421\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5493 - val_accuracy: 0.8421\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5671 - val_accuracy: 0.8421\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5395 - val_accuracy: 0.8421\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5667 - val_accuracy: 0.8421\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 14s 200ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6348 - val_accuracy: 0.8421\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 9.4240e-04 - accuracy: 1.0000 - val_loss: 0.5746 - val_accuracy: 0.8421\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 15s 203ms/step - loss: 8.9508e-04 - accuracy: 1.0000 - val_loss: 0.5869 - val_accuracy: 0.8421\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 14s 201ms/step - loss: 8.2826e-04 - accuracy: 1.0000 - val_loss: 0.6304 - val_accuracy: 0.8421\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 8.0077e-04 - accuracy: 1.0000 - val_loss: 0.5892 - val_accuracy: 0.8421\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 7.2680e-04 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 0.8421\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 15s 201ms/step - loss: 6.8333e-04 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 0.8421\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 6.3721e-04 - accuracy: 1.0000 - val_loss: 0.6399 - val_accuracy: 0.8421\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 15s 202ms/step - loss: 5.8505e-04 - accuracy: 1.0000 - val_loss: 0.7255 - val_accuracy: 0.7895\n",
      "Epoch 1\n",
      "max_val :0.8421052694320679\n",
      "save_acc :1.0\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 15s 204ms/step - loss: 0.8585 - accuracy: 0.5616 - val_loss: 0.6466 - val_accuracy: 0.6667\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 15s 201ms/step - loss: 0.6575 - accuracy: 0.5890 - val_loss: 0.6154 - val_accuracy: 0.6667\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 14s 198ms/step - loss: 0.5860 - accuracy: 0.6712 - val_loss: 0.6001 - val_accuracy: 0.6111\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 15s 200ms/step - loss: 0.5582 - accuracy: 0.7534 - val_loss: 0.6074 - val_accuracy: 0.5556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "73/73 [==============================] - 15s 201ms/step - loss: 0.5129 - accuracy: 0.7534 - val_loss: 0.6169 - val_accuracy: 0.7222\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 15s 201ms/step - loss: 0.4517 - accuracy: 0.7397 - val_loss: 0.6249 - val_accuracy: 0.7222\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 15s 200ms/step - loss: 0.4058 - accuracy: 0.7945 - val_loss: 0.6482 - val_accuracy: 0.6111\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 15s 200ms/step - loss: 0.4504 - accuracy: 0.7945 - val_loss: 0.6409 - val_accuracy: 0.6667\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 15s 204ms/step - loss: 0.3762 - accuracy: 0.8356 - val_loss: 0.9523 - val_accuracy: 0.4444\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.3138 - accuracy: 0.8493 - val_loss: 0.6738 - val_accuracy: 0.6111\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.2492 - accuracy: 0.9315 - val_loss: 0.5070 - val_accuracy: 0.7778\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.2206 - accuracy: 0.9178 - val_loss: 0.5585 - val_accuracy: 0.7222\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.1694 - accuracy: 0.9178 - val_loss: 0.6328 - val_accuracy: 0.6111\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.1212 - accuracy: 0.9863 - val_loss: 0.6200 - val_accuracy: 0.6111\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0728 - accuracy: 0.9863 - val_loss: 0.6226 - val_accuracy: 0.5556\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0724 - accuracy: 1.0000 - val_loss: 0.4114 - val_accuracy: 0.8333\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.4963 - val_accuracy: 0.7222\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0438 - accuracy: 0.9863 - val_loss: 0.4289 - val_accuracy: 0.7778\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.6111\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.5068 - val_accuracy: 0.7222\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.3901 - val_accuracy: 0.8333\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.5988 - val_accuracy: 0.6111\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.5005 - val_accuracy: 0.8333\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 15s 204ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.5748 - val_accuracy: 0.7778\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4817 - val_accuracy: 0.8333\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5317 - val_accuracy: 0.8333\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 15s 204ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.5768 - val_accuracy: 0.8333\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 0.8333\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7849 - val_accuracy: 0.6111\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.6402 - val_accuracy: 0.8333\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5574 - val_accuracy: 0.8333\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5095 - val_accuracy: 0.8333\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6573 - val_accuracy: 0.8333\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5704 - val_accuracy: 0.8333\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6584 - val_accuracy: 0.8333\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6626 - val_accuracy: 0.8333\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5996 - val_accuracy: 0.8333\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.8333\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 15s 201ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6627 - val_accuracy: 0.8333\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6575 - val_accuracy: 0.8333\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.8333\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6471 - val_accuracy: 0.8333\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 9.8996e-04 - accuracy: 1.0000 - val_loss: 0.7029 - val_accuracy: 0.8333\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 9.5927e-04 - accuracy: 1.0000 - val_loss: 0.6838 - val_accuracy: 0.8333\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 8.6713e-04 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.8333\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 15s 204ms/step - loss: 7.8558e-04 - accuracy: 1.0000 - val_loss: 0.6636 - val_accuracy: 0.8333\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 15s 203ms/step - loss: 7.2129e-04 - accuracy: 1.0000 - val_loss: 0.7508 - val_accuracy: 0.8333\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 15s 205ms/step - loss: 6.8812e-04 - accuracy: 1.0000 - val_loss: 0.6699 - val_accuracy: 0.8333\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 15s 202ms/step - loss: 6.6308e-04 - accuracy: 1.0000 - val_loss: 0.6431 - val_accuracy: 0.8333\n",
      "Epoch 2\n",
      "max_val :0.8333333134651184\n",
      "save_acc :1.0\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 17s 237ms/step - loss: 0.8959 - accuracy: 0.5479 - val_loss: 0.6893 - val_accuracy: 0.6667\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.6763 - accuracy: 0.5753 - val_loss: 0.6671 - val_accuracy: 0.5556\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.6111 - accuracy: 0.7123 - val_loss: 0.5918 - val_accuracy: 0.7222\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.5927 - accuracy: 0.6986 - val_loss: 0.5938 - val_accuracy: 0.6111\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 17s 230ms/step - loss: 0.4947 - accuracy: 0.7808 - val_loss: 0.6425 - val_accuracy: 0.7222\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.4820 - accuracy: 0.7945 - val_loss: 0.6944 - val_accuracy: 0.6111\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.4155 - accuracy: 0.7945 - val_loss: 0.5923 - val_accuracy: 0.6111\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.3238 - accuracy: 0.9041 - val_loss: 0.6254 - val_accuracy: 0.7778\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 17s 226ms/step - loss: 0.2798 - accuracy: 0.8767 - val_loss: 0.5818 - val_accuracy: 0.6111\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.2701 - accuracy: 0.9041 - val_loss: 0.5705 - val_accuracy: 0.6667\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.2421 - accuracy: 0.8904 - val_loss: 0.6609 - val_accuracy: 0.6667\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.1437 - accuracy: 0.9726 - val_loss: 0.6479 - val_accuracy: 0.6111\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.1114 - accuracy: 0.9863 - val_loss: 0.6078 - val_accuracy: 0.6667\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0921 - accuracy: 0.9589 - val_loss: 0.8422 - val_accuracy: 0.7222\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0622 - accuracy: 0.9863 - val_loss: 0.7000 - val_accuracy: 0.6111\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.7021 - val_accuracy: 0.6111\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.6900 - val_accuracy: 0.6667\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.6947 - val_accuracy: 0.6667\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.6967 - val_accuracy: 0.6667\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.7035 - val_accuracy: 0.6667\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.7500 - val_accuracy: 0.7778\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.7315 - val_accuracy: 0.6667\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.7300 - val_accuracy: 0.6667\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.7649 - val_accuracy: 0.6667\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.7700 - val_accuracy: 0.7222\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 16s 222ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.7690 - val_accuracy: 0.6667\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.7855 - val_accuracy: 0.6667\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7960 - val_accuracy: 0.7778\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.7823 - val_accuracy: 0.6667\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.7951 - val_accuracy: 0.6667\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.8247 - val_accuracy: 0.6667\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.8071 - val_accuracy: 0.7778\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.8057 - val_accuracy: 0.6667\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8095 - val_accuracy: 0.6667\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8212 - val_accuracy: 0.6667\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8255 - val_accuracy: 0.7778\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8333 - val_accuracy: 0.6667\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8410 - val_accuracy: 0.7778\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8629 - val_accuracy: 0.6667\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8464 - val_accuracy: 0.7778\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8716 - val_accuracy: 0.6667\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8710 - val_accuracy: 0.6667\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8629 - val_accuracy: 0.6667\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.8567 - val_accuracy: 0.7778\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 9.6036e-04 - accuracy: 1.0000 - val_loss: 0.8758 - val_accuracy: 0.8333\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 9.4979e-04 - accuracy: 1.0000 - val_loss: 0.8725 - val_accuracy: 0.7778\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 8.2068e-04 - accuracy: 1.0000 - val_loss: 0.8745 - val_accuracy: 0.7778\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 7.3327e-04 - accuracy: 1.0000 - val_loss: 0.8804 - val_accuracy: 0.7222\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 7.2426e-04 - accuracy: 1.0000 - val_loss: 0.8832 - val_accuracy: 0.7778\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 6.6119e-04 - accuracy: 1.0000 - val_loss: 0.8914 - val_accuracy: 0.7222\n",
      "Epoch 3\n",
      "max_val :0.8333333134651184\n",
      "save_acc :1.0\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 17s 237ms/step - loss: 0.9012 - accuracy: 0.5753 - val_loss: 0.6429 - val_accuracy: 0.6111\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 17s 231ms/step - loss: 0.6456 - accuracy: 0.6301 - val_loss: 0.6405 - val_accuracy: 0.6111\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 17s 232ms/step - loss: 0.6178 - accuracy: 0.6438 - val_loss: 0.6338 - val_accuracy: 0.6111\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 17s 234ms/step - loss: 0.6145 - accuracy: 0.6712 - val_loss: 0.6057 - val_accuracy: 0.6111\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 17s 234ms/step - loss: 0.5507 - accuracy: 0.6301 - val_loss: 0.5623 - val_accuracy: 0.6667\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.5580 - accuracy: 0.6986 - val_loss: 0.6227 - val_accuracy: 0.6667\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.5190 - accuracy: 0.7671 - val_loss: 0.5299 - val_accuracy: 0.7778\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.4373 - accuracy: 0.7808 - val_loss: 0.5226 - val_accuracy: 0.7222\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 17s 232ms/step - loss: 0.3874 - accuracy: 0.8356 - val_loss: 0.4898 - val_accuracy: 0.7778\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.3521 - accuracy: 0.8630 - val_loss: 0.5125 - val_accuracy: 0.7222\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.3196 - accuracy: 0.8493 - val_loss: 0.4059 - val_accuracy: 0.8333\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 17s 233ms/step - loss: 0.2638 - accuracy: 0.9315 - val_loss: 0.4011 - val_accuracy: 0.8333\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 17s 226ms/step - loss: 0.2287 - accuracy: 0.9452 - val_loss: 0.4521 - val_accuracy: 0.7222\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.1852 - accuracy: 0.9315 - val_loss: 0.5215 - val_accuracy: 0.7778\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.1422 - accuracy: 0.9589 - val_loss: 0.3250 - val_accuracy: 0.8889\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0987 - accuracy: 1.0000 - val_loss: 0.7340 - val_accuracy: 0.6667\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0830 - accuracy: 0.9863 - val_loss: 0.2953 - val_accuracy: 0.8889\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.2880 - val_accuracy: 0.8889\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.3270 - val_accuracy: 0.8889\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.3231 - val_accuracy: 0.8889\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 17s 229ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.2898 - val_accuracy: 0.8889\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.2953 - val_accuracy: 0.8889\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.3971 - val_accuracy: 0.8889\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.3789 - val_accuracy: 0.8889\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.8889\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.3142 - val_accuracy: 0.9444\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.3284 - val_accuracy: 0.9444\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4384 - val_accuracy: 0.8889\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.3843 - val_accuracy: 0.8889\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.4829 - val_accuracy: 0.8889\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.3854 - val_accuracy: 0.9444\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4000 - val_accuracy: 0.9444\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4523 - val_accuracy: 0.8889\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4293 - val_accuracy: 0.8889\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4529 - val_accuracy: 0.8889\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4326 - val_accuracy: 0.9444\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4453 - val_accuracy: 0.9444\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4365 - val_accuracy: 0.9444\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4497 - val_accuracy: 0.9444\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4516 - val_accuracy: 0.9444\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4518 - val_accuracy: 0.9444\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4546 - val_accuracy: 0.9444\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4603 - val_accuracy: 0.9444\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4888 - val_accuracy: 0.8889\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4821 - val_accuracy: 0.9444\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4724 - val_accuracy: 0.9444\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4810 - val_accuracy: 0.9444\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.9444\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 9.1033e-04 - accuracy: 1.0000 - val_loss: 0.5378 - val_accuracy: 0.8889\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 16s 223ms/step - loss: 8.6686e-04 - accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.9444\n",
      "Epoch 4\n",
      "max_val :0.9444444179534912\n",
      "save_acc :1.0\n",
      "(?, 224, 224, 3)\n",
      "self.input_shape_r\n",
      "(None, 28, 256)\n",
      "inputs\n",
      "(?, 28, 1, 256)\n",
      "dot product\n",
      "(?, 28, 1, 256)\n",
      "tanh\n",
      "(?, 28, 1, 256)\n",
      "combUV\n",
      "(?, 28, 1, 256)\n",
      "attention_weights\n",
      "(?, 28, 1, 1)\n",
      "attention_weights sum\n",
      "(?, 28, 1, 1)\n",
      "attention_weights last\n",
      "(?, 28, 1)\n",
      "output\n",
      "(?, 256)\n",
      "Train on 73 samples, validate on 18 samples\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 17s 232ms/step - loss: 0.7630 - accuracy: 0.5205 - val_loss: 0.6324 - val_accuracy: 0.6111\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.6135 - accuracy: 0.6575 - val_loss: 0.6225 - val_accuracy: 0.6667\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.6173 - accuracy: 0.6986 - val_loss: 0.5986 - val_accuracy: 0.6111\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 17s 229ms/step - loss: 0.5850 - accuracy: 0.6986 - val_loss: 0.5576 - val_accuracy: 0.7778\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.5002 - accuracy: 0.7397 - val_loss: 0.5288 - val_accuracy: 0.7778\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.4434 - accuracy: 0.8219 - val_loss: 0.6198 - val_accuracy: 0.7222\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.4401 - accuracy: 0.8219 - val_loss: 0.4579 - val_accuracy: 0.8889\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 16s 224ms/step - loss: 0.3675 - accuracy: 0.8356 - val_loss: 0.5940 - val_accuracy: 0.7222\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.4419 - accuracy: 0.7808 - val_loss: 0.4285 - val_accuracy: 0.8333\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.3095 - accuracy: 0.8630 - val_loss: 0.3523 - val_accuracy: 0.8889\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.2520 - accuracy: 0.8630 - val_loss: 0.3567 - val_accuracy: 0.8889\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.1280 - accuracy: 0.9863 - val_loss: 0.3661 - val_accuracy: 0.8889\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.1060 - accuracy: 0.9726 - val_loss: 0.4985 - val_accuracy: 0.8333\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0611 - accuracy: 1.0000 - val_loss: 0.8727 - val_accuracy: 0.6667\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.3914 - val_accuracy: 0.8889\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 17s 232ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.7188 - val_accuracy: 0.7778\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 16s 225ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.5175 - val_accuracy: 0.8889\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.3916 - val_accuracy: 0.8889\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.5281 - val_accuracy: 0.8889\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.5279 - val_accuracy: 0.8889\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 17s 230ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.4935 - val_accuracy: 0.8889\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.5226 - val_accuracy: 0.8889\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.4223 - val_accuracy: 0.8889\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5106 - val_accuracy: 0.8889\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.6329 - val_accuracy: 0.8333\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 0.8889\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4432 - val_accuracy: 0.8889\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6760 - val_accuracy: 0.8333\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 0.8889\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4906 - val_accuracy: 0.8889\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 17s 229ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 0.8889\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5005 - val_accuracy: 0.8889\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6060 - val_accuracy: 0.8889\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5189 - val_accuracy: 0.8889\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 0.8889\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5740 - val_accuracy: 0.8889\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.5283 - val_accuracy: 0.8889\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 9.6670e-04 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 0.8889\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 9.2798e-04 - accuracy: 1.0000 - val_loss: 0.6916 - val_accuracy: 0.8889\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 9.2425e-04 - accuracy: 1.0000 - val_loss: 0.6271 - val_accuracy: 0.8889\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 7.9752e-04 - accuracy: 1.0000 - val_loss: 0.5659 - val_accuracy: 0.8889\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 7.0264e-04 - accuracy: 1.0000 - val_loss: 0.5897 - val_accuracy: 0.8889\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 6.9932e-04 - accuracy: 1.0000 - val_loss: 0.5241 - val_accuracy: 0.8889\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 6.2117e-04 - accuracy: 1.0000 - val_loss: 0.5782 - val_accuracy: 0.8889\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 16s 226ms/step - loss: 5.7763e-04 - accuracy: 1.0000 - val_loss: 0.5632 - val_accuracy: 0.8889\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 5.4207e-04 - accuracy: 1.0000 - val_loss: 0.5712 - val_accuracy: 0.8889\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 17s 226ms/step - loss: 5.1325e-04 - accuracy: 1.0000 - val_loss: 0.6425 - val_accuracy: 0.8889\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 17s 227ms/step - loss: 4.6559e-04 - accuracy: 1.0000 - val_loss: 0.5881 - val_accuracy: 0.8889\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 16s 225ms/step - loss: 4.5325e-04 - accuracy: 1.0000 - val_loss: 0.6426 - val_accuracy: 0.8889\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 17s 228ms/step - loss: 4.1491e-04 - accuracy: 1.0000 - val_loss: 0.6414 - val_accuracy: 0.8889\n",
      "Epoch 5\n",
      "max_val :0.8888888955116272\n",
      "save_acc :1.0\n"
     ]
    }
   ],
   "source": [
    "val_acc_list = [];\n",
    "acc_list = [];\n",
    "epochs = 50;\n",
    "batch_size =1 ;\n",
    "kf = KFold(n_splits=5)\n",
    "ep=1;\n",
    "for train_index, test_index in kf.split(X):\n",
    "    x_train, x_test = X[train_index,:], X[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "    img_rows, img_cols = 128, 128\n",
    "\n",
    "    pooling = 'No'\n",
    "    num_sequence = 28;\n",
    "    include_top = False;\n",
    "\n",
    "    input_shape =  [num_sequence,img_rows, img_cols, 3];\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "    inputs = img_input\n",
    "\n",
    "\n",
    "    x = vgg16_head(inputs);\n",
    "\n",
    "    # Create model.\n",
    "    base_model = models.Model(inputs, x, name='vgg16')\n",
    "\n",
    "    weights ='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5';\n",
    "\n",
    "    base_model.load_weights(weights)\n",
    "\n",
    "\n",
    "    head_model = FCHeadNet.build(base_model, 1, 256)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=head_model)\n",
    "\n",
    "    Dont_Want_to_train_all = True;\n",
    "\n",
    "\n",
    "    if (Dont_Want_to_train_all):\n",
    "\n",
    "      for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "    max_val=-1;\n",
    "    save_acc=0;\n",
    "    val_acc=history.history['val_accuracy'];\n",
    "    acc=history.history['accuracy'];\n",
    "    for h in range(len(val_acc)):\n",
    "        if(max_val<=val_acc[h]):\n",
    "            max_val=val_acc[h];\n",
    "            save_acc=acc[h];\n",
    "    print(\"Epoch \"+str(ep))\n",
    "    ep=ep+1;\n",
    "    print(\"max_val :\"+str(max_val))\n",
    "    print(\"save_acc :\"+str(save_acc))\n",
    "    val_acc_list.append(max_val)\n",
    "    acc_list.append(save_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'fold')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc_list,'go-')\n",
    "plt.plot(acc_list,'bo-')\n",
    "plt.title('Model Accuracy 5-kold Crossvalidation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train accuracy 100.0%\n",
      "Average Validation accuracy 86.84210419654846%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Train accuracy \"+str(np.mean(acc_list)*100)+'%')\n",
    "\n",
    "print(\"Average Validation accuracy \"+str(np.mean(val_acc_list)*100)+'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8421052694320679,\n",
       " 0.8333333134651184,\n",
       " 0.8333333134651184,\n",
       " 0.9444444179534912,\n",
       " 0.8888888955116272]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
